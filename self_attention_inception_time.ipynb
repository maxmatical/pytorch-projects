{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "self attention inception_time.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOCc950auUT1mdPhKO8zrtD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxmatical/pytorch-projects/blob/master/self_attention_inception_time.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjKlcrQPb8Bn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import fastai\n",
        "from fastai.vision import *\n",
        "# from models import *\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkBSqfaZcURM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "act_fn = nn.ReLU(inplace=True)\n",
        "def conv(ni, nf, ks=3, stride=1, bias=False):\n",
        "    return nn.Conv1d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)\n",
        "\n",
        "\n",
        "\n",
        "class AdaptiveConcatPool1d(nn.Module):\n",
        "    \"Layer that concats `AdaptiveAvgPool1d` and `AdaptiveMaxPool1d`.\"\n",
        "    def __init__(self, sz:Optional[int]=None):\n",
        "        \"Output will be 2*sz or 2 if sz is None\"\n",
        "        super().__init__()\n",
        "        self.output_size = sz or 1\n",
        "        self.ap = nn.AdaptiveAvgPool1d(self.output_size)\n",
        "        self.mp = nn.AdaptiveMaxPool1d(self.output_size)\n",
        "    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)\n",
        "\n",
        "\n",
        "class Shortcut(Module):\n",
        "    \"Merge a shortcut with the result of the module by adding them. Adds Conv, BN and ReLU\"\n",
        "    def __init__(self, ni, nf, act_fn=act_fn): \n",
        "        self.act_fn=act_fn\n",
        "        self.conv=conv(ni, nf, 1)\n",
        "        self.bn=nn.BatchNorm1d(nf)\n",
        "    def forward(self, x): return act_fn(x + self.bn(self.conv(x.orig)))\n",
        "\n",
        "class InceptionModule(Module):\n",
        "    \"An inception module for TimeSeries, based on https://arxiv.org/pdf/1611.06455.pdf\"\n",
        "    def __init__(self, ni, nb_filters=32, kss=[39, 19, 9], bottleneck_size=32, stride=1):\n",
        "        if (bottleneck_size>0 and ni>1): self.bottleneck = conv(ni, bottleneck_size, 1, stride)\n",
        "        else: self.bottleneck = noop\n",
        "        self.convs = nn.ModuleList([conv(bottleneck_size if (bottleneck_size>1 and ni>1) else ni, nb_filters, ks) for ks in listify(kss)])\n",
        "        self.conv_bottle = nn.Sequential(nn.MaxPool1d(3, stride, padding=1), conv(ni, nb_filters, 1))\n",
        "        self.bn_relu = nn.Sequential(nn.BatchNorm1d((len(kss)+1)*nb_filters), nn.ReLU())\n",
        "    def forward(self, x):\n",
        "        bottled = self.bottleneck(x)\n",
        "        return self.bn_relu(torch.cat([c(bottled) for c in self.convs]+[self.conv_bottle(x)], dim=1))\n",
        "\n",
        "def create_inception(ni, nout, kss=[39, 19, 9], depth=6, bottleneck_size=32, nb_filters=32, head=True):\n",
        "    \"Inception time architecture\"\n",
        "    layers = []\n",
        "    n_ks = len(kss) + 1\n",
        "    for d in range(depth):\n",
        "        im = SequentialEx(InceptionModule(ni if d==0 else n_ks*nb_filters, kss=kss, bottleneck_size=bottleneck_size))\n",
        "        if d%3==2: im.append(Shortcut(n_ks*nb_filters, n_ks*nb_filters))      \n",
        "        layers.append(im)\n",
        "    head = [AdaptiveConcatPool1d(), Flatten(), nn.Linear(2*n_ks*nb_filters, nout)] if head else []\n",
        "    return  nn.Sequential(*layers, *head)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2LmqL7hciTo",
        "colab_type": "text"
      },
      "source": [
        "tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ytc_w5u7cfEl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "dc1a83b9-b135-469e-a114-a5de66230644"
      },
      "source": [
        "tmp = torch.randn((32, 2, 1024))\n",
        "\n",
        "# conv\n",
        "a = conv(2, 32, 3, 1) \n",
        "print(a(tmp).shape)\n",
        "\n",
        "\n",
        "b = InceptionModule(2, 32, kss=[3, 7, 9], bottleneck_size = 0)\n",
        "print(b(tmp).shape)\n",
        "\n",
        "c = create_inception(2, 10)\n",
        "print(c(tmp).shape)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 32, 1024])\n",
            "torch.Size([32, 128, 1024])\n",
            "torch.Size([32, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdBzuUxVdcRm",
        "colab_type": "text"
      },
      "source": [
        "adding self attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqigqZ-IcrDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# conv1d layer (used for self attention layer):\n",
        "def conv1d(ni, no, ks=1, stride=1, padding=0, bias=False):\n",
        "    \"Create and initialize a `nn.Conv1d` layer with spectral normalization.\"\n",
        "    conv = nn.Conv1d(ni, no, ks, stride=stride, padding=padding, bias=bias)\n",
        "    nn.init.kaiming_normal_(conv.weight)\n",
        "    if bias: conv.bias.data.zero_()\n",
        "    return nn.utils.spectral_norm(conv)\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    \"Self attention layer for after convolutions.\"\n",
        "    def __init__(self, n_channels):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.query = conv1d(n_channels, n_channels//8)\n",
        "        self.key   = conv1d(n_channels, n_channels//8)\n",
        "        self.value = conv1d(n_channels, n_channels)\n",
        "        self.gamma = nn.Parameter(torch.tensor([0.]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        #Notation from https://arxiv.org/pdf/1805.08318.pdf\n",
        "        size = x.size()\n",
        "        x = x.view(*size[:2],-1)\n",
        "        f,g,h = self.query(x),self.key(x),self.value(x)\n",
        "        beta = F.softmax(torch.bmm(f.permute(0,2,1).contiguous(), g), dim=1)\n",
        "        o = self.gamma * torch.bmm(h, beta) + x\n",
        "        return o.view(*size).contiguous()\n",
        "\n",
        "\n",
        "\n",
        "class InceptionModule(Module):\n",
        "    \"An inception module for TimeSeries, based on https://arxiv.org/pdf/1611.06455.pdf\"\n",
        "    def __init__(self, ni, nb_filters=32, kss=[39, 19, 9], bottleneck_size=32, stride=1, self_attention = False):\n",
        "        self.nb_filters = nb_filters\n",
        "        self.self_attention = self_attention\n",
        "        if (bottleneck_size>0 and ni>1): self.bottleneck = conv(ni, bottleneck_size, 1, stride)\n",
        "        else: self.bottleneck = noop\n",
        "        self.convs = nn.ModuleList([conv(bottleneck_size if (bottleneck_size>1 and ni>1) else ni, nb_filters, ks) for ks in listify(kss)])\n",
        "        self.conv_bottle = nn.Sequential(nn.MaxPool1d(3, stride, padding=1), conv(ni, nb_filters, 1))\n",
        "        self.bn_relu = nn.Sequential(nn.BatchNorm1d((len(kss)+1)*nb_filters), nn.ReLU())\n",
        "        self.attn = SelfAttention(self.nb_filters*4)\n",
        "    def forward(self, x):\n",
        "        bottled = self.bottleneck(x)\n",
        "        out =  self.bn_relu(torch.cat([c(bottled) for c in self.convs]+[self.conv_bottle(x)], dim=1))\n",
        "        if self.self_attention:\n",
        "            out = self.attn(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def create_inception(ni, nout, kss=[39, 19, 9], depth=6, bottleneck_size=32, nb_filters=32, head=True, self_attention = False):\n",
        "    \"Inception time architecture\"\n",
        "    layers = []\n",
        "    n_ks = len(kss) + 1\n",
        "    for d in range(depth):\n",
        "        im = SequentialEx(InceptionModule(ni if d==0 else n_ks*nb_filters, kss=kss, bottleneck_size=bottleneck_size, self_attention=self_attention))\n",
        "        if d%3==2: im.append(Shortcut(n_ks*nb_filters, n_ks*nb_filters))      \n",
        "        layers.append(im)\n",
        "    head = [AdaptiveConcatPool1d(), Flatten(), nn.Linear(2*n_ks*nb_filters, nout)] if head else []\n",
        "    return  nn.Sequential(*layers, *head)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snkfV9WJejut",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "7018549e-8849-4386-95b8-3a22ca6aeae2"
      },
      "source": [
        "tmp = torch.randn((32, 2, 1024))\n",
        "\n",
        "# conv\n",
        "a = conv(2, 32, 3, 1) \n",
        "print(a(tmp).shape)\n",
        "\n",
        "\n",
        "b = InceptionModule(2, self_attention = True)\n",
        "print(b(tmp).shape)\n",
        "\n",
        "c = create_inception(2, 10, self_attention = True)\n",
        "print(c(tmp).shape)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 32, 1024])\n",
            "torch.Size([32, 128, 1024])\n",
            "torch.Size([32, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l65pDBtBep2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}