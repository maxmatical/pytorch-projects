{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SENet_v3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/maxmatical/pytorch-projects/blob/master/SENet_v3.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "PxqP7my30faV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# install pytorch 0.4.1 with gpu\n",
        "\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s-wxtSvV0Znv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "c4541683-7550-470c-cf34-c087636038d0"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "print('Torch', torch.__version__, 'CUDA', torch.version.cuda)\n",
        "print('Device:', torch.device('cuda:0'))\n"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4.1\n",
            "True\n",
            "Torch 0.4.1 CUDA 8.0.61\n",
            "Device: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QPDIfPx20yxR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86df57d8-4785-4572-e34a-ac5b1cc0849b"
      },
      "cell_type": "code",
      "source": [
        "  \n",
        "# install required packagles\n",
        "!pip install torchsummary"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OP2vMQnp0RJ5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "#import ipdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cXlUnQyx0RJ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FKhQnFHU0RKA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "cdc20e93-97cb-4681-a6d5-aa4ff3fb8fe4"
      },
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "                                        \n",
        "                                        \n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
        "                                         shuffle=False, num_workers=2)\n"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gcNHP-Uukukn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1, padding = 1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=padding, bias=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tFyN_PLd0RKS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, n_channels, reduction_ratio = 16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Linear(n_channels, n_channels//reduction_ratio)\n",
        "        self.relu = nn.LeakyReLU(inplace = True)\n",
        "        self.fc2 = nn.Linear(n_channels//reduction_ratio, n_channels)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input):\n",
        "        b, c, _, _ = input.size()\n",
        "        out = self.avg_pool(input).view(b,c)\n",
        "        out = self.relu(self.fc1(out))\n",
        "        out = self.sigmoid(self.fc2(out))\n",
        "        out = out.view(b,c,1,1)\n",
        "        out = out*input\n",
        "\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hha121eRIll7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# basic block (optional preact)\n",
        "class SE_Res_Block(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None, reduction_ratio = 16, pre_act = False):\n",
        "        super(SE_Res_Block, self).__init__()\n",
        "        self.bn0 = nn.BatchNorm2d(in_channels)\n",
        "        self.conv1 = conv3x3(in_channels, out_channels, stride = stride)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(out_channels, out_channels, stride = 1)  #2nd conv is always stride 1\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.se = SEBlock(out_channels, reduction_ratio)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "        self.pre_act = pre_act\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        if self.pre_act == True:\n",
        "             # 1st conv layer \n",
        "            out = self.bn0(x)\n",
        "            out = self.relu(out)\n",
        "            out = self.conv1(out)\n",
        "            \n",
        "            # 2nd conv layer\n",
        "            out = self.bn1(out)\n",
        "            out = self.relu(out)\n",
        "            out = self.conv2(out)\n",
        "            # SEblock\n",
        "            out = self.se(out)\n",
        "            \n",
        "        else:\n",
        "            # 1st conv layer\n",
        "            out = self.conv1(x)\n",
        "            out = self.bn1(out)\n",
        "            out = self.relu(out)\n",
        "            # 2nd conv layer\n",
        "            out = self.conv2(out)\n",
        "            out = self.bn2(out)\n",
        "            #out = self.relu(out)\n",
        "            out = self.se(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4E0NQY3T0RKV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# bottleneck block (optional pre_activations)\n",
        "class SE_Res_Block_Bottleneck(nn.Module): \n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None, reduction_ratio = 16, pre_act = False):    \n",
        "        super(SE_Res_Block_Bottleneck, self).__init__()\n",
        "        self.bn0 = nn.BatchNorm2d(in_channels)\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 1, bias=False) \n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = conv3x3(out_channels, out_channels, stride = stride) # 3x3 conv with stride = stride\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, 1, bias=False) \n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels* self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample \n",
        "        self.stride = stride \n",
        "        self.reduction_ratio = reduction_ratio\n",
        "        self.se = SEBlock(out_channels * self.expansion, reduction_ratio)\n",
        "        self.pre_act = pre_act\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        if self.pre_act == True:\n",
        "            #1x1 block\n",
        "            out = self.bn0(x)\n",
        "            out = self.relu(out)\n",
        "            out = self.conv1(out)\n",
        "            \n",
        "            # downsampling block (3x3 conv)\n",
        "            out = self.bn1(out)\n",
        "            out = self.relu(out)\n",
        "            out = self.conv2(out)\n",
        "            \n",
        "            # expand block (1x1 conv)\n",
        "            out = self.bn2(out)\n",
        "            out = self.relu(out)\n",
        "            out = self.conv3(out)\n",
        "            \n",
        "            # SEblock\n",
        "            out = self.se(out)\n",
        "            \n",
        "        else:\n",
        "            #1x1block\n",
        "            out = self.conv1(x)\n",
        "            out = self.bn1(out)\n",
        "            out = self.relu(out)\n",
        "            \n",
        "            # downsampling block (3x3 conv)\n",
        "            out = self.conv2(out)\n",
        "            out = self.bn2(out)\n",
        "            out = self.relu(out)\n",
        "            \n",
        "            # expand block (1x1 conv)\n",
        "            out = self.conv3(out)\n",
        "            out = self.bn3(out)\n",
        "            \n",
        "            #SEBlock\n",
        "            out = self.se(out)\n",
        "            \n",
        "            \n",
        "        # adjust for dimension mismatch\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "\n",
        "\n",
        "        return out\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BJVdGshL0RKX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# SENet\n",
        "n_classes = 10 \n",
        "\n",
        "class SENet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes = n_classes, reduction_ratio = 16): # layer is a list\n",
        "        super(SENet, self).__init__()\n",
        "        #initial conv layer\n",
        "        self.conv1 = nn.Conv2d(3, 16, 7, stride=1, padding = 3, bias = False) # initial conv layer similar to resnet\n",
        "        self.in_channels = 16 # match outchannel for conv1\n",
        "        self.batchnorm1 = nn.BatchNorm2d(16) # match outchannel for conv1\n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        ################\n",
        "        self.layer1 = self.make_layer(block, 16, layers[0], reduction_ratio = reduction_ratio)\n",
        "        self.layer2 = self.make_layer(block, 32, layers[1], stride = 2, reduction_ratio = reduction_ratio)\n",
        "        self.layer3 = self.make_layer(block, 64, layers[2], stride = 2, reduction_ratio = reduction_ratio)\n",
        "        self.layer4 = self.make_layer(block, 128, layers[3], stride = 2, reduction_ratio = reduction_ratio)\n",
        "        ################\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.maxpool = nn.AdaptiveMaxPool2d(1) \n",
        "        self.linear = nn.Linear(128*block.expansion, num_classes) # in_features = out_channel from last layer * expansion\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "    \n",
        "    \n",
        "    #######################\n",
        "    # make layers\n",
        "    #######################\n",
        "    \n",
        "    def make_layer(self, block, out_channels, blocks, reduction_ratio, stride=1):\n",
        "        # block = residual_block\n",
        "        # out_channel = output dimension of the block\n",
        "        # blocks = number of residual_block to use\n",
        "        # stride = stride length\n",
        "\n",
        "        downsample = None\n",
        "\n",
        "        # if dimesions don't match up\n",
        "        if (stride != 1) or (self.in_channels != out_channels * block.expansion):\n",
        "            downsample = nn.Sequential(\n",
        "            nn.Conv2d(self.in_channels, out_channels*block.expansion, kernel_size=1, stride = stride, bias = False),\n",
        "            nn.BatchNorm2d(out_channels*block.expansion))\n",
        "\n",
        "        # creating multiple layers of resblocks\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, downsample,  reduction_ratio))\n",
        "        self.in_channels = out_channels*block.expansion\n",
        "\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # initial conv layer to improve starting point\n",
        "        out = self.conv1(x)\n",
        "        out = self.batchnorm1(out)\n",
        "        out = self.relu(out)\n",
        "        #out = self.maxpool(out)\n",
        "        # resblocks\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avg_pool(out) # adaptive avg pooling to get (-1, out_channel(last layer), 1, 1)\n",
        "        out = out.view(out.size(0), -1) # flatten\n",
        "        out = self.linear(out) # output layer\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rELlW067Q4ih",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Preact_SENet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes = n_classes, reduction_ratio = 16): # layer is a list\n",
        "        super(Preact_SENet, self).__init__()\n",
        "        #initial conv layer\n",
        "        self.conv1 = nn.Conv2d(3, 16, 7, stride=1, padding = 3, bias = False) # initial conv layer similar to resnet\n",
        "        self.in_channels = 16 # match outchannel for conv1\n",
        "        self.batchnorm1 = nn.BatchNorm2d(16) # match outchannel for conv1\n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        ################\n",
        "        self.layer1 = self.make_layer(block, 16, layers[0], stride = 1, reduction_ratio = reduction_ratio, pre_act= True)\n",
        "        self.layer2 = self.make_layer(block, 32, layers[1], stride = 2, reduction_ratio = reduction_ratio, pre_act= True)\n",
        "        self.layer3 = self.make_layer(block, 64, layers[2], stride = 2, reduction_ratio = reduction_ratio, pre_act= True)\n",
        "        self.layer4 = self.make_layer(block, 128, layers[3], stride = 2, reduction_ratio = reduction_ratio, pre_act= True)\n",
        "        ################\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.maxpool = nn.AdaptiveMaxPool2d(1) \n",
        "        self.linear = nn.Linear(128*block.expansion, num_classes) # in_features = out_channel from last layer * expansion\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "    \n",
        "    #######################\n",
        "    # make layers\n",
        "    #######################\n",
        "    \n",
        "    def make_layer(self, block, out_channels, blocks, reduction_ratio, stride=1, pre_act= True):\n",
        "        # block = residual_block\n",
        "        # out_channel = output dimension of the block\n",
        "        # blocks = number of residual_block to use\n",
        "        # stride = stride length\n",
        "\n",
        "        downsample = None\n",
        "\n",
        "        # if dimesions don't match up\n",
        "        if (stride != 1) or (self.in_channels != out_channels * block.expansion):\n",
        "            downsample = nn.Sequential(\n",
        "            nn.Conv2d(self.in_channels, out_channels*block.expansion, kernel_size=1, stride = stride, bias = False),\n",
        "            nn.BatchNorm2d(out_channels*block.expansion))\n",
        "\n",
        "        # creating multiple layers of resblocks\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, downsample,  reduction_ratio, pre_act= True))\n",
        "        self.in_channels = out_channels*block.expansion\n",
        "\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # initial conv layer to improve starting point\n",
        "        out = self.conv1(x)\n",
        "        out = self.batchnorm1(out)\n",
        "        out = self.relu(out)\n",
        "        #out = self.maxpool(out)\n",
        "        # resblocks\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avg_pool(out) # adaptive avg pooling to get (-1, out_channel(last layer), 1, 1)\n",
        "        out = out.view(out.size(0), -1) # flatten\n",
        "        out = self.linear(out) # output layer\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MwTcMnPm0RKa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "6c3ffaf1-07ba-4b84-9155-e0ed7e0f1568"
      },
      "cell_type": "code",
      "source": [
        "net1 = SENet(SE_Res_Block, layers = [2,2,2,2])\n",
        "net2 = Preact_SENet(SE_Res_Block, layers = [2,2,2,2])\n",
        "net3 = SENet(SE_Res_Block_Bottleneck, layers = [1,1,1,1])\n",
        "net4 = Preact_SENet(SE_Res_Block_Bottleneck, layers = [1,1,1,1])\n",
        "\n",
        "use_cuda = True\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    net1.cuda()\n",
        "    net2.cuda()\n",
        "    net3.cuda()\n",
        "    net4.cuda()\n",
        "# check if models are on cuda\n",
        "print(next(net1.parameters()).is_cuda)\n",
        "print(next(net2.parameters()).is_cuda)\n",
        "print(next(net3.parameters()).is_cuda)\n",
        "print(next(net4.parameters()).is_cuda)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BF8Glbm6Q19E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2184
        },
        "outputId": "b8ec90ab-8abe-4392-f662-ec8737ea8b33"
      },
      "cell_type": "code",
      "source": [
        "summary(net1, (3,32,32))"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 32, 32]           2,352\n",
            "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
            "              ReLU-3           [-1, 16, 32, 32]               0\n",
            "            Conv2d-4           [-1, 16, 32, 32]           2,304\n",
            "       BatchNorm2d-5           [-1, 16, 32, 32]              32\n",
            "              ReLU-6           [-1, 16, 32, 32]               0\n",
            "            Conv2d-7           [-1, 16, 32, 32]           2,304\n",
            "       BatchNorm2d-8           [-1, 16, 32, 32]              32\n",
            " AdaptiveAvgPool2d-9             [-1, 16, 1, 1]               0\n",
            "           Linear-10                    [-1, 1]              17\n",
            "        LeakyReLU-11                    [-1, 1]               0\n",
            "           Linear-12                   [-1, 16]              32\n",
            "          Sigmoid-13                   [-1, 16]               0\n",
            "          SEBlock-14           [-1, 16, 32, 32]               0\n",
            "             ReLU-15           [-1, 16, 32, 32]               0\n",
            "     SE_Res_Block-16           [-1, 16, 32, 32]               0\n",
            "           Conv2d-17           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-18           [-1, 16, 32, 32]              32\n",
            "             ReLU-19           [-1, 16, 32, 32]               0\n",
            "           Conv2d-20           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-21           [-1, 16, 32, 32]              32\n",
            "AdaptiveAvgPool2d-22             [-1, 16, 1, 1]               0\n",
            "           Linear-23                    [-1, 1]              17\n",
            "        LeakyReLU-24                    [-1, 1]               0\n",
            "           Linear-25                   [-1, 16]              32\n",
            "          Sigmoid-26                   [-1, 16]               0\n",
            "          SEBlock-27           [-1, 16, 32, 32]               0\n",
            "             ReLU-28           [-1, 16, 32, 32]               0\n",
            "     SE_Res_Block-29           [-1, 16, 32, 32]               0\n",
            "           Conv2d-30           [-1, 32, 16, 16]           4,608\n",
            "      BatchNorm2d-31           [-1, 32, 16, 16]              64\n",
            "             ReLU-32           [-1, 32, 16, 16]               0\n",
            "           Conv2d-33           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-34           [-1, 32, 16, 16]              64\n",
            "AdaptiveAvgPool2d-35             [-1, 32, 1, 1]               0\n",
            "           Linear-36                    [-1, 2]              66\n",
            "        LeakyReLU-37                    [-1, 2]               0\n",
            "           Linear-38                   [-1, 32]              96\n",
            "          Sigmoid-39                   [-1, 32]               0\n",
            "          SEBlock-40           [-1, 32, 16, 16]               0\n",
            "           Conv2d-41           [-1, 32, 16, 16]             512\n",
            "      BatchNorm2d-42           [-1, 32, 16, 16]              64\n",
            "             ReLU-43           [-1, 32, 16, 16]               0\n",
            "     SE_Res_Block-44           [-1, 32, 16, 16]               0\n",
            "           Conv2d-45           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-46           [-1, 32, 16, 16]              64\n",
            "             ReLU-47           [-1, 32, 16, 16]               0\n",
            "           Conv2d-48           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-49           [-1, 32, 16, 16]              64\n",
            "AdaptiveAvgPool2d-50             [-1, 32, 1, 1]               0\n",
            "           Linear-51                    [-1, 2]              66\n",
            "        LeakyReLU-52                    [-1, 2]               0\n",
            "           Linear-53                   [-1, 32]              96\n",
            "          Sigmoid-54                   [-1, 32]               0\n",
            "          SEBlock-55           [-1, 32, 16, 16]               0\n",
            "             ReLU-56           [-1, 32, 16, 16]               0\n",
            "     SE_Res_Block-57           [-1, 32, 16, 16]               0\n",
            "           Conv2d-58             [-1, 64, 8, 8]          18,432\n",
            "      BatchNorm2d-59             [-1, 64, 8, 8]             128\n",
            "             ReLU-60             [-1, 64, 8, 8]               0\n",
            "           Conv2d-61             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-62             [-1, 64, 8, 8]             128\n",
            "AdaptiveAvgPool2d-63             [-1, 64, 1, 1]               0\n",
            "           Linear-64                    [-1, 4]             260\n",
            "        LeakyReLU-65                    [-1, 4]               0\n",
            "           Linear-66                   [-1, 64]             320\n",
            "          Sigmoid-67                   [-1, 64]               0\n",
            "          SEBlock-68             [-1, 64, 8, 8]               0\n",
            "           Conv2d-69             [-1, 64, 8, 8]           2,048\n",
            "      BatchNorm2d-70             [-1, 64, 8, 8]             128\n",
            "             ReLU-71             [-1, 64, 8, 8]               0\n",
            "     SE_Res_Block-72             [-1, 64, 8, 8]               0\n",
            "           Conv2d-73             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-74             [-1, 64, 8, 8]             128\n",
            "             ReLU-75             [-1, 64, 8, 8]               0\n",
            "           Conv2d-76             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-77             [-1, 64, 8, 8]             128\n",
            "AdaptiveAvgPool2d-78             [-1, 64, 1, 1]               0\n",
            "           Linear-79                    [-1, 4]             260\n",
            "        LeakyReLU-80                    [-1, 4]               0\n",
            "           Linear-81                   [-1, 64]             320\n",
            "          Sigmoid-82                   [-1, 64]               0\n",
            "          SEBlock-83             [-1, 64, 8, 8]               0\n",
            "             ReLU-84             [-1, 64, 8, 8]               0\n",
            "     SE_Res_Block-85             [-1, 64, 8, 8]               0\n",
            "           Conv2d-86            [-1, 128, 4, 4]          73,728\n",
            "      BatchNorm2d-87            [-1, 128, 4, 4]             256\n",
            "             ReLU-88            [-1, 128, 4, 4]               0\n",
            "           Conv2d-89            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-90            [-1, 128, 4, 4]             256\n",
            "AdaptiveAvgPool2d-91            [-1, 128, 1, 1]               0\n",
            "           Linear-92                    [-1, 8]           1,032\n",
            "        LeakyReLU-93                    [-1, 8]               0\n",
            "           Linear-94                  [-1, 128]           1,152\n",
            "          Sigmoid-95                  [-1, 128]               0\n",
            "          SEBlock-96            [-1, 128, 4, 4]               0\n",
            "           Conv2d-97            [-1, 128, 4, 4]           8,192\n",
            "      BatchNorm2d-98            [-1, 128, 4, 4]             256\n",
            "             ReLU-99            [-1, 128, 4, 4]               0\n",
            "    SE_Res_Block-100            [-1, 128, 4, 4]               0\n",
            "          Conv2d-101            [-1, 128, 4, 4]         147,456\n",
            "     BatchNorm2d-102            [-1, 128, 4, 4]             256\n",
            "            ReLU-103            [-1, 128, 4, 4]               0\n",
            "          Conv2d-104            [-1, 128, 4, 4]         147,456\n",
            "     BatchNorm2d-105            [-1, 128, 4, 4]             256\n",
            "AdaptiveAvgPool2d-106            [-1, 128, 1, 1]               0\n",
            "          Linear-107                    [-1, 8]           1,032\n",
            "       LeakyReLU-108                    [-1, 8]               0\n",
            "          Linear-109                  [-1, 128]           1,152\n",
            "         Sigmoid-110                  [-1, 128]               0\n",
            "         SEBlock-111            [-1, 128, 4, 4]               0\n",
            "            ReLU-112            [-1, 128, 4, 4]               0\n",
            "    SE_Res_Block-113            [-1, 128, 4, 4]               0\n",
            "AdaptiveAvgPool2d-114            [-1, 128, 1, 1]               0\n",
            "          Linear-115                   [-1, 10]           1,290\n",
            "         Dropout-116                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 709,336\n",
            "Trainable params: 709,336\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 4.36\n",
            "Params size (MB): 2.71\n",
            "Estimated Total Size (MB): 7.07\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PWJMuDtsPw4N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "summary(net2, (3, 32, 32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yO10I2PP0RKc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1462
        },
        "outputId": "cfe8ce2e-1050-4d48-9e79-243c5c5a965d"
      },
      "cell_type": "code",
      "source": [
        "# summary\n",
        "summary(net3, (3, 32, 32))\n"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 32, 32]           2,352\n",
            "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
            "              ReLU-3           [-1, 16, 32, 32]               0\n",
            "            Conv2d-4           [-1, 16, 32, 32]             256\n",
            "       BatchNorm2d-5           [-1, 16, 32, 32]              32\n",
            "              ReLU-6           [-1, 16, 32, 32]               0\n",
            "            Conv2d-7           [-1, 16, 32, 32]           2,304\n",
            "       BatchNorm2d-8           [-1, 16, 32, 32]              32\n",
            "              ReLU-9           [-1, 16, 32, 32]               0\n",
            "           Conv2d-10           [-1, 64, 32, 32]           1,024\n",
            "      BatchNorm2d-11           [-1, 64, 32, 32]             128\n",
            "AdaptiveAvgPool2d-12             [-1, 64, 1, 1]               0\n",
            "           Linear-13                    [-1, 4]             260\n",
            "        LeakyReLU-14                    [-1, 4]               0\n",
            "           Linear-15                   [-1, 64]             320\n",
            "          Sigmoid-16                   [-1, 64]               0\n",
            "          SEBlock-17           [-1, 64, 32, 32]               0\n",
            "           Conv2d-18           [-1, 64, 32, 32]           1,024\n",
            "      BatchNorm2d-19           [-1, 64, 32, 32]             128\n",
            "SE_Res_Block_Bottleneck-20           [-1, 64, 32, 32]               0\n",
            "           Conv2d-21           [-1, 32, 32, 32]           2,048\n",
            "      BatchNorm2d-22           [-1, 32, 32, 32]              64\n",
            "             ReLU-23           [-1, 32, 32, 32]               0\n",
            "           Conv2d-24           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-25           [-1, 32, 16, 16]              64\n",
            "             ReLU-26           [-1, 32, 16, 16]               0\n",
            "           Conv2d-27          [-1, 128, 16, 16]           4,096\n",
            "      BatchNorm2d-28          [-1, 128, 16, 16]             256\n",
            "AdaptiveAvgPool2d-29            [-1, 128, 1, 1]               0\n",
            "           Linear-30                    [-1, 8]           1,032\n",
            "        LeakyReLU-31                    [-1, 8]               0\n",
            "           Linear-32                  [-1, 128]           1,152\n",
            "          Sigmoid-33                  [-1, 128]               0\n",
            "          SEBlock-34          [-1, 128, 16, 16]               0\n",
            "           Conv2d-35          [-1, 128, 16, 16]           8,192\n",
            "      BatchNorm2d-36          [-1, 128, 16, 16]             256\n",
            "SE_Res_Block_Bottleneck-37          [-1, 128, 16, 16]               0\n",
            "           Conv2d-38           [-1, 64, 16, 16]           8,192\n",
            "      BatchNorm2d-39           [-1, 64, 16, 16]             128\n",
            "             ReLU-40           [-1, 64, 16, 16]               0\n",
            "           Conv2d-41             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-42             [-1, 64, 8, 8]             128\n",
            "             ReLU-43             [-1, 64, 8, 8]               0\n",
            "           Conv2d-44            [-1, 256, 8, 8]          16,384\n",
            "      BatchNorm2d-45            [-1, 256, 8, 8]             512\n",
            "AdaptiveAvgPool2d-46            [-1, 256, 1, 1]               0\n",
            "           Linear-47                   [-1, 16]           4,112\n",
            "        LeakyReLU-48                   [-1, 16]               0\n",
            "           Linear-49                  [-1, 256]           4,352\n",
            "          Sigmoid-50                  [-1, 256]               0\n",
            "          SEBlock-51            [-1, 256, 8, 8]               0\n",
            "           Conv2d-52            [-1, 256, 8, 8]          32,768\n",
            "      BatchNorm2d-53            [-1, 256, 8, 8]             512\n",
            "SE_Res_Block_Bottleneck-54            [-1, 256, 8, 8]               0\n",
            "           Conv2d-55            [-1, 128, 8, 8]          32,768\n",
            "      BatchNorm2d-56            [-1, 128, 8, 8]             256\n",
            "             ReLU-57            [-1, 128, 8, 8]               0\n",
            "           Conv2d-58            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-59            [-1, 128, 4, 4]             256\n",
            "             ReLU-60            [-1, 128, 4, 4]               0\n",
            "           Conv2d-61            [-1, 512, 4, 4]          65,536\n",
            "      BatchNorm2d-62            [-1, 512, 4, 4]           1,024\n",
            "AdaptiveAvgPool2d-63            [-1, 512, 1, 1]               0\n",
            "           Linear-64                   [-1, 32]          16,416\n",
            "        LeakyReLU-65                   [-1, 32]               0\n",
            "           Linear-66                  [-1, 512]          16,896\n",
            "          Sigmoid-67                  [-1, 512]               0\n",
            "          SEBlock-68            [-1, 512, 4, 4]               0\n",
            "           Conv2d-69            [-1, 512, 4, 4]         131,072\n",
            "      BatchNorm2d-70            [-1, 512, 4, 4]           1,024\n",
            "SE_Res_Block_Bottleneck-71            [-1, 512, 4, 4]               0\n",
            "AdaptiveAvgPool2d-72            [-1, 512, 1, 1]               0\n",
            "           Linear-73                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 556,054\n",
            "Trainable params: 556,054\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 8.42\n",
            "Params size (MB): 2.12\n",
            "Estimated Total Size (MB): 10.55\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4ByVbE1jWbtS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1529
        },
        "outputId": "203fb831-d07f-48f3-cbee-7b6bbe683400"
      },
      "cell_type": "code",
      "source": [
        "summary(net4, (3, 32, 32))\n"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 32, 32]           2,352\n",
            "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
            "              ReLU-3           [-1, 16, 32, 32]               0\n",
            "       BatchNorm2d-4           [-1, 16, 32, 32]              32\n",
            "              ReLU-5           [-1, 16, 32, 32]               0\n",
            "            Conv2d-6           [-1, 16, 32, 32]             256\n",
            "       BatchNorm2d-7           [-1, 16, 32, 32]              32\n",
            "              ReLU-8           [-1, 16, 32, 32]               0\n",
            "            Conv2d-9           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-10           [-1, 16, 32, 32]              32\n",
            "             ReLU-11           [-1, 16, 32, 32]               0\n",
            "           Conv2d-12           [-1, 64, 32, 32]           1,024\n",
            "AdaptiveAvgPool2d-13             [-1, 64, 1, 1]               0\n",
            "           Linear-14                    [-1, 4]             260\n",
            "        LeakyReLU-15                    [-1, 4]               0\n",
            "           Linear-16                   [-1, 64]             320\n",
            "          Sigmoid-17                   [-1, 64]               0\n",
            "          SEBlock-18           [-1, 64, 32, 32]               0\n",
            "           Conv2d-19           [-1, 64, 32, 32]           1,024\n",
            "      BatchNorm2d-20           [-1, 64, 32, 32]             128\n",
            "SE_Res_Block_Bottleneck-21           [-1, 64, 32, 32]               0\n",
            "      BatchNorm2d-22           [-1, 64, 32, 32]             128\n",
            "             ReLU-23           [-1, 64, 32, 32]               0\n",
            "           Conv2d-24           [-1, 32, 32, 32]           2,048\n",
            "      BatchNorm2d-25           [-1, 32, 32, 32]              64\n",
            "             ReLU-26           [-1, 32, 32, 32]               0\n",
            "           Conv2d-27           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-28           [-1, 32, 16, 16]              64\n",
            "             ReLU-29           [-1, 32, 16, 16]               0\n",
            "           Conv2d-30          [-1, 128, 16, 16]           4,096\n",
            "AdaptiveAvgPool2d-31            [-1, 128, 1, 1]               0\n",
            "           Linear-32                    [-1, 8]           1,032\n",
            "        LeakyReLU-33                    [-1, 8]               0\n",
            "           Linear-34                  [-1, 128]           1,152\n",
            "          Sigmoid-35                  [-1, 128]               0\n",
            "          SEBlock-36          [-1, 128, 16, 16]               0\n",
            "           Conv2d-37          [-1, 128, 16, 16]           8,192\n",
            "      BatchNorm2d-38          [-1, 128, 16, 16]             256\n",
            "SE_Res_Block_Bottleneck-39          [-1, 128, 16, 16]               0\n",
            "      BatchNorm2d-40          [-1, 128, 16, 16]             256\n",
            "             ReLU-41          [-1, 128, 16, 16]               0\n",
            "           Conv2d-42           [-1, 64, 16, 16]           8,192\n",
            "      BatchNorm2d-43           [-1, 64, 16, 16]             128\n",
            "             ReLU-44           [-1, 64, 16, 16]               0\n",
            "           Conv2d-45             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-46             [-1, 64, 8, 8]             128\n",
            "             ReLU-47             [-1, 64, 8, 8]               0\n",
            "           Conv2d-48            [-1, 256, 8, 8]          16,384\n",
            "AdaptiveAvgPool2d-49            [-1, 256, 1, 1]               0\n",
            "           Linear-50                   [-1, 16]           4,112\n",
            "        LeakyReLU-51                   [-1, 16]               0\n",
            "           Linear-52                  [-1, 256]           4,352\n",
            "          Sigmoid-53                  [-1, 256]               0\n",
            "          SEBlock-54            [-1, 256, 8, 8]               0\n",
            "           Conv2d-55            [-1, 256, 8, 8]          32,768\n",
            "      BatchNorm2d-56            [-1, 256, 8, 8]             512\n",
            "SE_Res_Block_Bottleneck-57            [-1, 256, 8, 8]               0\n",
            "      BatchNorm2d-58            [-1, 256, 8, 8]             512\n",
            "             ReLU-59            [-1, 256, 8, 8]               0\n",
            "           Conv2d-60            [-1, 128, 8, 8]          32,768\n",
            "      BatchNorm2d-61            [-1, 128, 8, 8]             256\n",
            "             ReLU-62            [-1, 128, 8, 8]               0\n",
            "           Conv2d-63            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-64            [-1, 128, 4, 4]             256\n",
            "             ReLU-65            [-1, 128, 4, 4]               0\n",
            "           Conv2d-66            [-1, 512, 4, 4]          65,536\n",
            "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
            "           Linear-68                   [-1, 32]          16,416\n",
            "        LeakyReLU-69                   [-1, 32]               0\n",
            "           Linear-70                  [-1, 512]          16,896\n",
            "          Sigmoid-71                  [-1, 512]               0\n",
            "          SEBlock-72            [-1, 512, 4, 4]               0\n",
            "           Conv2d-73            [-1, 512, 4, 4]         131,072\n",
            "      BatchNorm2d-74            [-1, 512, 4, 4]           1,024\n",
            "SE_Res_Block_Bottleneck-75            [-1, 512, 4, 4]               0\n",
            "AdaptiveAvgPool2d-76            [-1, 512, 1, 1]               0\n",
            "           Linear-77                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 555,062\n",
            "Trainable params: 555,062\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 9.48\n",
            "Params size (MB): 2.12\n",
            "Estimated Total Size (MB): 11.61\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NUgoOUVH0RKr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# weight initialization\n",
        "# new init weight        \n",
        "def init_weight(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight) #he initialize, can use xavier instead\n",
        "        #nn.init.constant_(m.bias, 0.001) # optional bias\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_normal_(m.weight) #he initialize, can use xavier instead\n",
        "        #nn.init.constant_(m.bias, 0.001) # optional bias\n",
        "    elif type(m) == nn.BatchNorm2d:\n",
        "        torch.nn.init.constant_(m.weight, 1)\n",
        "        torch.nn.init.constant_(m.bias, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J8TpiUWt0RKt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2503
        },
        "outputId": "9bcad434-87aa-4748-b46e-d9459e8bf0ef"
      },
      "cell_type": "code",
      "source": [
        "# apply initializers\n",
        "net1.apply(init_weight)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SENet(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
              "  (batchnorm1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace)\n",
              "  (layer1): Sequential(\n",
              "    (0): SE_Res_Block(\n",
              "      (bn0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (se): SEBlock(\n",
              "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "        (fc1): Linear(in_features=16, out_features=1, bias=True)\n",
              "        (relu): LeakyReLU(negative_slope=0.01, inplace)\n",
              "        (fc2): Linear(in_features=1, out_features=16, bias=True)\n",
              "        (sigmoid): Sigmoid()\n",
              "      )\n",
              "    )\n",
              "    (1): SE_Res_Block(\n",
              "      (bn0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (se): SEBlock(\n",
              "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "        (fc1): Linear(in_features=16, out_features=1, bias=True)\n",
              "        (relu): LeakyReLU(negative_slope=0.01, inplace)\n",
              "        (fc2): Linear(in_features=1, out_features=16, bias=True)\n",
              "        (sigmoid): Sigmoid()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): SE_Res_Block(\n",
              "      (bn0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (se): SEBlock(\n",
              "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "        (fc1): Linear(in_features=32, out_features=2, bias=True)\n",
              "        (relu): LeakyReLU(negative_slope=0.01, inplace)\n",
              "        (fc2): Linear(in_features=2, out_features=32, bias=True)\n",
              "        (sigmoid): Sigmoid()\n",
              "      )\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): SE_Res_Block(\n",
              "      (bn0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (se): SEBlock(\n",
              "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "        (fc1): Linear(in_features=32, out_features=2, bias=True)\n",
              "        (relu): LeakyReLU(negative_slope=0.01, inplace)\n",
              "        (fc2): Linear(in_features=2, out_features=32, bias=True)\n",
              "        (sigmoid): Sigmoid()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): SE_Res_Block(\n",
              "      (bn0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (se): SEBlock(\n",
              "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "        (fc1): Linear(in_features=64, out_features=4, bias=True)\n",
              "        (relu): LeakyReLU(negative_slope=0.01, inplace)\n",
              "        (fc2): Linear(in_features=4, out_features=64, bias=True)\n",
              "        (sigmoid): Sigmoid()\n",
              "      )\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): SE_Res_Block(\n",
              "      (bn0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (se): SEBlock(\n",
              "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "        (fc1): Linear(in_features=64, out_features=4, bias=True)\n",
              "        (relu): LeakyReLU(negative_slope=0.01, inplace)\n",
              "        (fc2): Linear(in_features=4, out_features=64, bias=True)\n",
              "        (sigmoid): Sigmoid()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): SE_Res_Block(\n",
              "      (bn0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (se): SEBlock(\n",
              "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "        (fc1): Linear(in_features=128, out_features=8, bias=True)\n",
              "        (relu): LeakyReLU(negative_slope=0.01, inplace)\n",
              "        (fc2): Linear(in_features=8, out_features=128, bias=True)\n",
              "        (sigmoid): Sigmoid()\n",
              "      )\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): SE_Res_Block(\n",
              "      (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (se): SEBlock(\n",
              "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "        (fc1): Linear(in_features=128, out_features=8, bias=True)\n",
              "        (relu): LeakyReLU(negative_slope=0.01, inplace)\n",
              "        (fc2): Linear(in_features=8, out_features=128, bias=True)\n",
              "        (sigmoid): Sigmoid()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "  (maxpool): AdaptiveMaxPool2d(output_size=1)\n",
              "  (linear): Linear(in_features=128, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "metadata": {
        "id": "OsjqWehH0RKw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define loss and optimizer\n",
        "import torch.optim as optim\n",
        "learning_rate = 0.01\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr = learning_rate, momentum=0.9, nesterov= True, weight_decay= 0.01)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qpybaz9_0RKz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# LR scheduler\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "\n",
        "class CosineAnnealingLR_with_Restart(_LRScheduler):\n",
        "    \"\"\"Set the learning rate of each parameter group using a cosine annealing\n",
        "    schedule, where :math:`\\eta_{max}` is set to the initial lr and\n",
        "    :math:`T_{cur}` is the number of epochs since the last restart in SGDR:\n",
        "\n",
        "    .. math::\n",
        "\n",
        "        \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 +\n",
        "        \\cos(\\frac{T_{cur}}{T_{max}}\\pi))\n",
        "\n",
        "    When last_epoch=-1, sets initial lr as lr.\n",
        "\n",
        "    It has been proposed in\n",
        "    `SGDR: Stochastic Gradient Descent with Warm Restarts`_. The original pytorch\n",
        "    implementation only implements the cosine annealing part of SGDR,\n",
        "    I added my own implementation of the restarts part.\n",
        "    \n",
        "    Args:\n",
        "        optimizer (Optimizer): Wrapped optimizer.\n",
        "        T_max (int): Maximum number of iterations. (LENGTH OF 1 CYCLE)\n",
        "        T_mult (float): Increase T_max by a factor of T_mult\n",
        "        eta_min (float): Minimum learning rate. Default: 0.\n",
        "        last_epoch (int): The index of last epoch. Default: -1.\n",
        "        model (pytorch model): The model to save.\n",
        "        out_dir (str): Directory to save snapshots\n",
        "        take_snapshot (bool): Whether to save snapshots at every restart\n",
        "\n",
        "    .. _SGDR\\: Stochastic Gradient Descent with Warm Restarts:\n",
        "        https://arxiv.org/abs/1608.03983\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, optimizer, T_max, T_mult, model, out_dir, take_snapshot, eta_min=0, last_epoch=-1):\n",
        "        self.T_max = T_max\n",
        "        self.T_mult = T_mult\n",
        "        self.Te = self.T_max\n",
        "        self.eta_min = eta_min\n",
        "        self.current_epoch = last_epoch\n",
        "        \n",
        "        self.model = model\n",
        "        self.out_dir = out_dir\n",
        "        self.take_snapshot = take_snapshot\n",
        "        \n",
        "        self.lr_history = []\n",
        "        \n",
        "        super(CosineAnnealingLR_with_Restart, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        new_lrs = [self.eta_min + (base_lr - self.eta_min) *\n",
        "                (1 + math.cos(math.pi * self.current_epoch / self.Te)) / 2\n",
        "\n",
        "                for base_lr in self.base_lrs]\n",
        "        \n",
        "        self.lr_history.append(new_lrs)\n",
        "        return new_lrs\n",
        "    \n",
        "    def step(self, epoch=None):\n",
        "        if epoch is None:\n",
        "        \n",
        "            epoch = self.last_epoch + 1\n",
        "        self.last_epoch = epoch\n",
        "        self.current_epoch += 1\n",
        "        \n",
        "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
        "            param_group['lr'] = lr\n",
        "        \n",
        "        ## restart\n",
        "        if self.current_epoch == self.Te:\n",
        "            print(\"restart at epoch {:03d}\".format(self.last_epoch + 1))\n",
        "            \n",
        "            if self.take_snapshot:\n",
        "                torch.save({\n",
        "                    'epoch': self.T_max,\n",
        "                    'state_dict': self.model.state_dict()\n",
        "                }, self.out_dir + \"/\" + 'snapshot_e_{:03d}.pth.tar'.format(self.T_max))\n",
        "            \n",
        "            ## reset epochs since the last reset\n",
        "            self.current_epoch = 0\n",
        "            \n",
        "            ## reset the next goal\n",
        "            self.Te = int(self.Te * self.T_mult)\n",
        "            self.T_max = self.T_max + self.Te"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vTqqSPzw0RK2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# T_max = how many Epochs before restarting learning rate\n",
        "# T_mult = increase cycle length after restart \n",
        "\n",
        "# try:\n",
        "# 1st training cycle: T_max = 3, T_mult = 1 for 3 cycles (9 epochs)\n",
        "# 2nd training cycle: T_max = 3, T_mult = 2 for 3 cycles (21 epochs)\n",
        "\n",
        "scheduler = CosineAnnealingLR_with_Restart(optimizer, T_max=2, T_mult=2, model = net,  out_dir='blank', take_snapshot=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2njsc51I0RK7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# modified model training to keep track of train/val loss\n",
        "n_epochs = 6\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    scheduler.step()\n",
        "    running_loss = 0.0\n",
        "    total_train_loss = 0.0\n",
        "    for i, train_data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = train_data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print loss per n minibatches\n",
        "        running_loss += loss.item()\n",
        "        total_train_loss += loss.item()\n",
        "        if i % 500 == 499:    # print every 500 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 500))\n",
        "            running_loss = 0.0\n",
        "    \n",
        "    # keep track of loss in test dataset \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for test_data in testloader:\n",
        "            test_images, test_labels = test_data\n",
        "            test_outputs = net(test_images)\n",
        "            test_loss = criterion(test_outputs, test_labels)\n",
        "            total_test_loss += test_loss.item()\n",
        "            _, predicted = torch.max(test_outputs.data, 1)\n",
        "            total += test_labels.size(0)\n",
        "            correct += (predicted == test_labels).sum().item()\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    # for printing average loss every epoch\n",
        "    print(\"===> Epoch {} Complete: Train Avg. Loss: {:.4f}\".format(epoch+1, total_train_loss / len(trainloader)))\n",
        "    print(\"===> Epoch {} Complete: Test Avg. Loss: {:.4f}\".format(epoch+1, total_test_loss / len(testloader)))\n",
        "    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5MExUtlg0RK_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
