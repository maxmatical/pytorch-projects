{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and normalizing cifar10\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new transform\n",
    "#transform = transforms.Compose(\n",
    "#    [transforms.ToTensor(),\n",
    "#    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "#    transforms.Pad(4), \n",
    "#    transforms.RandomHorizontalFlip(),\n",
    "#    transforms.RandomCrop(32)])\n",
    "\n",
    "# old transform\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "                                        \n",
    "                                        \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
    "                                         shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating a function that initializes weights\n",
    "def init_weight_conv(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        torch.nn.init.kaiming_normal_(m.weight) #he initialize, can use xavier instead\n",
    "        #m.bias.data.fill_(0.001) # optional bias\n",
    "        \n",
    "def init_weight_linear(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        #m.bias.data.fill_(0.001)\n",
    "\n",
    "# new init weight        \n",
    "def init_weight(m):\n",
    "    if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
    "        torch.nn.init.kaiming_normal_(m.weight) #he initialize, can use xavier instead\n",
    "        #m.bias.data.fill_(0.001) # optional bias\n",
    "    elif type(m) == nn.BatchNorm2d:\n",
    "        torch.nn.init.constant_(m.weight, 1)\n",
    "        torch.nn.init.constant_(m.bias, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual block\n",
    "class residual_block(nn.Module):\n",
    "    expansion = 1 # used in downsampling\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n",
    "        super(residual_block, self).__init__()\n",
    "        # conv1 has defined stride\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride = stride, padding = 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding = 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample \n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        # 1st conv layer\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        # 2nd conv layer\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "        \n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bottleneck block\n",
    "\n",
    "class bottleneck(nn.Module):\n",
    "    expansion = 4 # used in downsampling\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n",
    "        super(bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 1, bias=False) # no stride\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False) # has stride argument       \n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels*self.expansion, 1, bias=False) #expanding out_channels\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels*self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample \n",
    "        self.stride = stride # why is this used?\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet\n",
    "n_classes = 10 \n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes = n_classes): # layer is a list\n",
    "        super(ResNet, self).__init__()\n",
    "        #initial conv layer\n",
    "        self.conv1 = nn.Conv2d(3, 16, 7, stride=1, padding = 3, bias = False) # first thing in resnet\n",
    "        self.in_channels = 16 # match outchannel for conv1\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16) # match outchannel for conv1\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        #self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1) # outchannel still the same\n",
    "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 32, layers[1],stride = 2)\n",
    "        self.layer3 = self.make_layer(block, 64, layers[2], stride = 2)\n",
    "        self.layer4 = self.make_layer(block, 128, layers[3], stride = 2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.linear = nn.Linear(128*block.expansion, num_classes) # in_features = out_channel from last layer * expansion\n",
    "        \n",
    "        \n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        # block = residual_block\n",
    "        # out_channel = output dimension of the block\n",
    "        # blocks = number of residual_block to use\n",
    "        # stride = stride length\n",
    "\n",
    "        downsample = None\n",
    "\n",
    "        # if dimesions don't match up\n",
    "        if (stride != 1) or (self.in_channels != out_channels * block.expansion):\n",
    "            downsample = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, out_channels*block.expansion, kernel_size=1, stride = stride, bias = False),\n",
    "            nn.BatchNorm2d(out_channels*block.expansion))\n",
    "\n",
    "        # creating multiple layers of resblocks\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels*block.expansion\n",
    "\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # initial conv layer to improve starting point\n",
    "        out = self.conv1(x)\n",
    "        out = self.batchnorm1(out)\n",
    "        out = self.relu(out)\n",
    "        #out = self.maxpool(out)\n",
    "        # resblocks\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avg_pool(out) # adaptive avg pooling to get (-1, out_channel(last layer), 1, 1)\n",
    "        out = out.view(out.size(0), -1) # flatten\n",
    "        out = self.linear(out) # output layer\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smaller resnet\n",
    "n_classes = 10\n",
    "\n",
    "class ResNet_small(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes = n_classes): # layer is a list\n",
    "        super(ResNet_small, self).__init__()\n",
    "        #initial conv layer\n",
    "        self.conv1 = nn.Conv2d(3, 4, 7, stride=2, padding = 3, bias = False) # first thing in resnet\n",
    "        self.in_channels = 4 # match outchannel for conv1\n",
    "        self.batchnorm1 = nn.BatchNorm2d(4) # match outchannel for conv1\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        #self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1) # outchannel still the same\n",
    "        self.layer1 = self.make_layer(block, 4, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 8, layers[1],stride = 2)\n",
    "        self.layer3 = self.make_layer(block, 16, layers[2], stride = 2)\n",
    "        self.layer4 = self.make_layer(block, 32, layers[3], stride = 2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.linear = nn.Linear(32*block.expansion, num_classes) # in_features = out_channel from last layer * expansion\n",
    "        \n",
    "        \n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        # block = residual_block\n",
    "        # out_channel = output dimension of the block\n",
    "        # blocks = number of residual_block to use\n",
    "        # stride = stride length\n",
    "\n",
    "        downsample = None\n",
    "\n",
    "        # if dimesions don't match up\n",
    "        if (stride != 1) or (self.in_channels != out_channels * block.expansion):\n",
    "            downsample = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, out_channels*block.expansion, kernel_size=1, stride = stride, bias = False),\n",
    "            nn.BatchNorm2d(out_channels*block.expansion))\n",
    "\n",
    "        # creating multiple layers of resblocks\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels*block.expansion\n",
    "\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # initial conv layer to improve starting point\n",
    "        out = self.conv1(x)\n",
    "        out = self.batchnorm1(out)\n",
    "        out = self.relu(out)\n",
    "        #out = self.maxpool(out)\n",
    "        # resblocks\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avg_pool(out) # adaptive avg pooling to get (-1, out_channel(last layer), 1, 1)\n",
    "        out = out.view(out.size(0), -1) # flatten\n",
    "        out = self.linear(out) # output layer\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify different network architectures\n",
    "\n",
    "#net = ResNet(residual_block, layers = [2,2,2,2])\n",
    "\n",
    "#net = ResNet(bottleneck, layers = [2,2,2,2])\n",
    "\n",
    "net = ResNet_small(residual_block, layers = [1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 4, 16, 16]             588\n",
      "       BatchNorm2d-2            [-1, 4, 16, 16]               8\n",
      "              ReLU-3            [-1, 4, 16, 16]               0\n",
      "            Conv2d-4            [-1, 4, 16, 16]             144\n",
      "       BatchNorm2d-5            [-1, 4, 16, 16]               8\n",
      "              ReLU-6            [-1, 4, 16, 16]               0\n",
      "            Conv2d-7            [-1, 4, 16, 16]             144\n",
      "       BatchNorm2d-8            [-1, 4, 16, 16]               8\n",
      "              ReLU-9            [-1, 4, 16, 16]               0\n",
      "   residual_block-10            [-1, 4, 16, 16]               0\n",
      "           Conv2d-11              [-1, 8, 8, 8]             288\n",
      "      BatchNorm2d-12              [-1, 8, 8, 8]              16\n",
      "             ReLU-13              [-1, 8, 8, 8]               0\n",
      "           Conv2d-14              [-1, 8, 8, 8]             576\n",
      "      BatchNorm2d-15              [-1, 8, 8, 8]              16\n",
      "           Conv2d-16              [-1, 8, 8, 8]              32\n",
      "      BatchNorm2d-17              [-1, 8, 8, 8]              16\n",
      "             ReLU-18              [-1, 8, 8, 8]               0\n",
      "   residual_block-19              [-1, 8, 8, 8]               0\n",
      "           Conv2d-20             [-1, 16, 4, 4]           1,152\n",
      "      BatchNorm2d-21             [-1, 16, 4, 4]              32\n",
      "             ReLU-22             [-1, 16, 4, 4]               0\n",
      "           Conv2d-23             [-1, 16, 4, 4]           2,304\n",
      "      BatchNorm2d-24             [-1, 16, 4, 4]              32\n",
      "           Conv2d-25             [-1, 16, 4, 4]             128\n",
      "      BatchNorm2d-26             [-1, 16, 4, 4]              32\n",
      "             ReLU-27             [-1, 16, 4, 4]               0\n",
      "   residual_block-28             [-1, 16, 4, 4]               0\n",
      "           Conv2d-29             [-1, 32, 2, 2]           4,608\n",
      "      BatchNorm2d-30             [-1, 32, 2, 2]              64\n",
      "             ReLU-31             [-1, 32, 2, 2]               0\n",
      "           Conv2d-32             [-1, 32, 2, 2]           9,216\n",
      "      BatchNorm2d-33             [-1, 32, 2, 2]              64\n",
      "           Conv2d-34             [-1, 32, 2, 2]             512\n",
      "      BatchNorm2d-35             [-1, 32, 2, 2]              64\n",
      "             ReLU-36             [-1, 32, 2, 2]               0\n",
      "   residual_block-37             [-1, 32, 2, 2]               0\n",
      "AdaptiveAvgPool2d-38             [-1, 32, 1, 1]               0\n",
      "           Linear-39                   [-1, 10]             330\n",
      "================================================================\n",
      "Total params: 20,382\n",
      "Trainable params: 20,382\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# visualize network\n",
    "#for i, weights in enumerate(list(net.parameters())):\n",
    "#    print('i:',i,'weights:',weights.size())\n",
    "\n",
    "#print(net)\n",
    "\n",
    "summary(net, (3, 32, 32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet_small(\n",
       "  (conv1): Conv2d(3, 4, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (batchnorm1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (layer1): Sequential(\n",
       "    (0): residual_block(\n",
       "      (conv1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): residual_block(\n",
       "      (conv1): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(4, 8, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): residual_block(\n",
       "      (conv1): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): residual_block(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (linear): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply initializers\n",
    "net.apply(init_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss and optimizer\n",
    "import torch.optim as optim\n",
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 2.936\n",
      "[1,   100] loss: 1.971\n",
      "[1,   150] loss: 1.905\n",
      "[1,   200] loss: 1.834\n",
      "[1,   250] loss: 1.812\n",
      "[1,   300] loss: 1.778\n",
      "[1,   350] loss: 1.740\n",
      "[1,   400] loss: 1.720\n",
      "[1,   450] loss: 1.684\n",
      "[1,   500] loss: 1.689\n",
      "[1,   550] loss: 1.665\n",
      "[1,   600] loss: 1.658\n",
      "[1,   650] loss: 1.621\n",
      "[1,   700] loss: 1.588\n",
      "[1,   750] loss: 1.592\n",
      "===> Epoch 1 Complete: Train Avg. Loss: 1.8046\n",
      "===> Epoch 1 Complete: Test Avg. Loss: 1.5564\n",
      "Accuracy of the network on the 10000 test images: 44 %\n",
      "[2,    50] loss: 1.567\n",
      "[2,   100] loss: 1.523\n",
      "[2,   150] loss: 1.542\n",
      "[2,   200] loss: 1.522\n",
      "[2,   250] loss: 1.505\n",
      "[2,   300] loss: 1.505\n",
      "[2,   350] loss: 1.502\n",
      "[2,   400] loss: 1.469\n",
      "[2,   450] loss: 1.479\n",
      "[2,   500] loss: 1.442\n",
      "[2,   550] loss: 1.484\n",
      "[2,   600] loss: 1.428\n",
      "[2,   650] loss: 1.453\n",
      "[2,   700] loss: 1.446\n",
      "[2,   750] loss: 1.458\n",
      "===> Epoch 2 Complete: Train Avg. Loss: 1.4856\n",
      "===> Epoch 2 Complete: Test Avg. Loss: 1.4326\n",
      "Accuracy of the network on the 10000 test images: 48 %\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# modified model training to keep track of train/val loss\n",
    "n_epochs = 2\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    total_train_loss = 0.0\n",
    "    for i, train_data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = train_data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print loss per n minibatches\n",
    "        running_loss += loss.item()\n",
    "        total_train_loss += loss.item()\n",
    "        if i % 50 == 49:    # print every 50 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 50))\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    # keep track of loss in test dataset \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for test_data in testloader:\n",
    "            test_images, test_labels = test_data\n",
    "            test_outputs = net(test_images)\n",
    "            test_loss = criterion(test_outputs, test_labels)\n",
    "            total_test_loss += test_loss.item()\n",
    "            _, predicted = torch.max(test_outputs.data, 1)\n",
    "            total += test_labels.size(0)\n",
    "            correct += (predicted == test_labels).sum().item()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # for printing average loss every epoch\n",
    "    print(\"===> Epoch {} Complete: Train Avg. Loss: {:.4f}\".format(epoch+1, total_train_loss / len(trainloader)))\n",
    "    print(\"===> Epoch {} Complete: Test Avg. Loss: {:.4f}\".format(epoch+1, total_test_loss / len(testloader)))\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
