{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention-Augmented Wide Resnet",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxmatical/pytorch-projects/blob/master/Attention_Augmented_Wide_Resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agKUVad7lJag",
        "colab_type": "code",
        "outputId": "1915ffe1-ba66-40f8-d005-0f12d5c4ee7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from attention_augmented_convnets import augmented_conv2d\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 20, 16, 16])\n",
            "16 16 256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJLOKQLQl9Xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            torch.nn.init.zeros_(m.bias)\n",
        "    elif isinstance(m, nn.BatchNorm2d):\n",
        "        m.weight.data.fill_(1)\n",
        "        m.bias.data.zero_()\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        n = m.weight.size(1)\n",
        "        m.weight.data.normal_(0, 0.01)\n",
        "        m.bias.data.zero_()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd_RLqmhmWtN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class wide_basic(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout_rate, stride = 1, v = 0.2, k = 2, n_heads = 4):\n",
        "        super(wide_basic, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        self.conv1 = augmented_conv2d(in_channels, out_channels, kernel_size=3, dk=k*out_channels, dv = int(v*out_channels), Nh= n_heads,\\\n",
        "                                     relative = True, padding = 1)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = augmented_conv2d(out_channels, out_channels, kernel_size=3, dk=k*out_channels, dv = int(v*out_channels), Nh= n_heads,\\\n",
        "                                    stride = stride, relative = True, padding = 1)\n",
        "        \n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride !=1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(augmented_conv2d(in_channels, out_channels, kernel_size = 3, dk=k*out_channels, dv = int(v*out_channels),\\\n",
        "                                         Nh = n_heads, relative = True, stride = stride, padding = 1),)\n",
        "            \n",
        "    def forward(self,x):\n",
        "        out = self.dropout(self.conv1(F.relu(self.bn1(x))))\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        out += self.shortcut(x)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HNitAGhqKso",
        "colab_type": "code",
        "outputId": "9e443216-cd76-47b8-8c28-82be9f09ae67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "tmp = torch.randn((16, 3, 32, 32)).to(device)\n",
        "a = wide_basic(3, 20, dropout_rate = 0.1).to(device)\n",
        "print(a(tmp).shape)\n",
        "bs, n_channels, H, W = a(tmp).size()\n",
        "print(H, W, H*W)\n",
        "\n",
        "a2 = wide_basic(3, 20, dropout_rate = 0.1, stride = 2).to(device)\n",
        "print(a2(tmp).shape)\n",
        "bs, n_channels, H, W = a2(tmp).size()\n",
        "print(H, W, H*W)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 20, 32, 32])\n",
            "32 32 1024\n",
            "torch.Size([16, 20, 16, 16])\n",
            "16 16 256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XAWldlmqWYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WideResnet(nn.Module):\n",
        "    def __init__(self, depth, widen_factor, dropout_rate, n_classes, layers):\n",
        "        \"\"\"\n",
        "        \n",
        "        layers should be a list of length 4\n",
        "        eg [20, 20, 40, 80] NEEDS TO BE >20 FOR split_heads_2d\n",
        "        layers will be multiplied by widen_factor to get out_channels for each block \n",
        "        shape = dimension of the image (shape x shape)\n",
        "        \n",
        "        \"\"\"\n",
        "        super(WideResnet, self).__init__()\n",
        "        self.in_channels = 20\n",
        "        \n",
        "        assert ((depth-4)%6 ==0), 'Wide-resnet depth should be 6n+4'\n",
        "        n = int((depth-4)/6) # each wide_basic block will have n conv layers\n",
        "        k = widen_factor\n",
        "        \n",
        "        dv_v = 0.2\n",
        "        dk_k = 2\n",
        "        Nh = 4\n",
        "        \n",
        "        self.conv1 = augmented_conv2d(3, out_channels = layers[0], kernel_size = 3, dk = dk_k*layers[0], dv = int(dv_v * layers[0]),\\\n",
        "                                     Nh = Nh, relative = True)\n",
        "        self.block1 = nn.Sequential(self.make_layer(wide_basic, layers[1]*k, n, dropout_rate, stride = 1),) # 1st block keeps same dimensions\n",
        "        self.block2 = nn.Sequential(self.make_layer(wide_basic, layers[2]*k, n, dropout_rate, stride = 2),) # 2nd block reduces dimensions by 1/2\n",
        "        self.block3 = nn.Sequential(self.make_layer(wide_basic, layers[3]*k, n, dropout_rate, stride = 2),)\n",
        "        self.bn1 = nn.BatchNorm2d(layers[3]*k, momentum=0.9)\n",
        "        self.linear = nn.Linear(layers[3]*k*2, n_classes) #*2 because we use both avg pool and max pool\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        \n",
        "        self.apply(init_weights)\n",
        "        \n",
        "    def make_layer(self, block, out_channels, n_blocks, dropout_rate, stride):\n",
        "        strides = [stride] + [1]*(n_blocks-1)\n",
        "        layers = []\n",
        "        \n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, dropout_rate, stride = stride))\n",
        "            self.in_channels = out_channels\n",
        "            \n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = F.relu(self.bn1(self.block3(out)))\n",
        "        a = self.avg_pool(out)\n",
        "        b = self.max_pool(out)\n",
        "        a = a.view(a.size(0),-1) # flatten\n",
        "        b = b.view(b.size(0),-1) # flatten\n",
        "        out = a.view(out.size(0), -1)\n",
        "        out = torch.cat([a,b],1)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15lum4quPWLA",
        "colab_type": "code",
        "outputId": "84fa1d3c-f6ea-40f2-d44a-746491a19183",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tmp = torch.randn((4, 3, 32, 32)).to(device)\n",
        "layers=[20, 20, 40, 40]\n",
        "\n",
        "model = WideResnet(28, 10, 0.3, 10, layers).to(device)\n",
        "print(model(tmp).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k489T_Sy2j04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = WideResnet(28, 4, 0.3, 10, [20, 20, 40, 80]).to(device)\n",
        "# res_block = "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2C6rw682eyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchsummary import summary\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJmqdgVt27gQ",
        "colab_type": "code",
        "outputId": "55ba86db-e462-4d16-f93c-ee77e09298a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2990
        }
      },
      "source": [
        "summary(net, (3, 32, 32))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 32, 32]             448\n",
            "            Conv2d-2           [-1, 84, 32, 32]           2,352\n",
            "            Conv2d-3            [-1, 4, 32, 32]              20\n",
            "  augmented_conv2d-4           [-1, 20, 32, 32]               0\n",
            "       BatchNorm2d-5           [-1, 20, 32, 32]              40\n",
            "            Conv2d-6           [-1, 64, 32, 32]          11,584\n",
            "            Conv2d-7          [-1, 336, 32, 32]          60,816\n",
            "            Conv2d-8           [-1, 16, 32, 32]             272\n",
            "  augmented_conv2d-9           [-1, 80, 32, 32]               0\n",
            "          Dropout-10           [-1, 80, 32, 32]               0\n",
            "      BatchNorm2d-11           [-1, 80, 32, 32]             160\n",
            "           Conv2d-12           [-1, 64, 32, 32]          46,144\n",
            "           Conv2d-13          [-1, 336, 32, 32]         242,256\n",
            "           Conv2d-14           [-1, 16, 32, 32]             272\n",
            " augmented_conv2d-15           [-1, 80, 32, 32]               0\n",
            "           Conv2d-16           [-1, 64, 32, 32]          11,584\n",
            "           Conv2d-17          [-1, 336, 32, 32]          60,816\n",
            "           Conv2d-18           [-1, 16, 32, 32]             272\n",
            " augmented_conv2d-19           [-1, 80, 32, 32]               0\n",
            "       wide_basic-20           [-1, 80, 32, 32]               0\n",
            "      BatchNorm2d-21           [-1, 80, 32, 32]             160\n",
            "           Conv2d-22           [-1, 64, 32, 32]          46,144\n",
            "           Conv2d-23          [-1, 336, 32, 32]         242,256\n",
            "           Conv2d-24           [-1, 16, 32, 32]             272\n",
            " augmented_conv2d-25           [-1, 80, 32, 32]               0\n",
            "          Dropout-26           [-1, 80, 32, 32]               0\n",
            "      BatchNorm2d-27           [-1, 80, 32, 32]             160\n",
            "           Conv2d-28           [-1, 64, 32, 32]          46,144\n",
            "           Conv2d-29          [-1, 336, 32, 32]         242,256\n",
            "           Conv2d-30           [-1, 16, 32, 32]             272\n",
            " augmented_conv2d-31           [-1, 80, 32, 32]               0\n",
            "       wide_basic-32           [-1, 80, 32, 32]               0\n",
            "      BatchNorm2d-33           [-1, 80, 32, 32]             160\n",
            "           Conv2d-34           [-1, 64, 32, 32]          46,144\n",
            "           Conv2d-35          [-1, 336, 32, 32]         242,256\n",
            "           Conv2d-36           [-1, 16, 32, 32]             272\n",
            " augmented_conv2d-37           [-1, 80, 32, 32]               0\n",
            "          Dropout-38           [-1, 80, 32, 32]               0\n",
            "      BatchNorm2d-39           [-1, 80, 32, 32]             160\n",
            "           Conv2d-40           [-1, 64, 32, 32]          46,144\n",
            "           Conv2d-41          [-1, 336, 32, 32]         242,256\n",
            "           Conv2d-42           [-1, 16, 32, 32]             272\n",
            " augmented_conv2d-43           [-1, 80, 32, 32]               0\n",
            "       wide_basic-44           [-1, 80, 32, 32]               0\n",
            "      BatchNorm2d-45           [-1, 80, 32, 32]             160\n",
            "           Conv2d-46           [-1, 64, 32, 32]          46,144\n",
            "           Conv2d-47          [-1, 336, 32, 32]         242,256\n",
            "           Conv2d-48           [-1, 16, 32, 32]             272\n",
            " augmented_conv2d-49           [-1, 80, 32, 32]               0\n",
            "          Dropout-50           [-1, 80, 32, 32]               0\n",
            "      BatchNorm2d-51           [-1, 80, 32, 32]             160\n",
            "           Conv2d-52           [-1, 64, 32, 32]          46,144\n",
            "           Conv2d-53          [-1, 336, 32, 32]         242,256\n",
            "           Conv2d-54           [-1, 16, 32, 32]             272\n",
            " augmented_conv2d-55           [-1, 80, 32, 32]               0\n",
            "       wide_basic-56           [-1, 80, 32, 32]               0\n",
            "      BatchNorm2d-57           [-1, 80, 32, 32]             160\n",
            "           Conv2d-58          [-1, 128, 32, 32]          92,288\n",
            "           Conv2d-59          [-1, 672, 32, 32]         484,512\n",
            "           Conv2d-60           [-1, 32, 32, 32]           1,056\n",
            " augmented_conv2d-61          [-1, 160, 32, 32]               0\n",
            "          Dropout-62          [-1, 160, 32, 32]               0\n",
            "      BatchNorm2d-63          [-1, 160, 32, 32]             320\n",
            "           Conv2d-64          [-1, 128, 16, 16]         184,448\n",
            "           Conv2d-65          [-1, 672, 16, 16]         968,352\n",
            "           Conv2d-66           [-1, 32, 16, 16]           1,056\n",
            " augmented_conv2d-67          [-1, 160, 16, 16]               0\n",
            "           Conv2d-68          [-1, 128, 16, 16]          92,288\n",
            "           Conv2d-69          [-1, 672, 16, 16]         484,512\n",
            "           Conv2d-70           [-1, 32, 16, 16]           1,056\n",
            " augmented_conv2d-71          [-1, 160, 16, 16]               0\n",
            "       wide_basic-72          [-1, 160, 16, 16]               0\n",
            "      BatchNorm2d-73          [-1, 160, 16, 16]             320\n",
            "           Conv2d-74          [-1, 128, 16, 16]         184,448\n",
            "           Conv2d-75          [-1, 672, 16, 16]         968,352\n",
            "           Conv2d-76           [-1, 32, 16, 16]           1,056\n",
            " augmented_conv2d-77          [-1, 160, 16, 16]               0\n",
            "          Dropout-78          [-1, 160, 16, 16]               0\n",
            "      BatchNorm2d-79          [-1, 160, 16, 16]             320\n",
            "           Conv2d-80          [-1, 128, 16, 16]         184,448\n",
            "           Conv2d-81          [-1, 672, 16, 16]         968,352\n",
            "           Conv2d-82           [-1, 32, 16, 16]           1,056\n",
            " augmented_conv2d-83          [-1, 160, 16, 16]               0\n",
            "       wide_basic-84          [-1, 160, 16, 16]               0\n",
            "      BatchNorm2d-85          [-1, 160, 16, 16]             320\n",
            "           Conv2d-86          [-1, 128, 16, 16]         184,448\n",
            "           Conv2d-87          [-1, 672, 16, 16]         968,352\n",
            "           Conv2d-88           [-1, 32, 16, 16]           1,056\n",
            " augmented_conv2d-89          [-1, 160, 16, 16]               0\n",
            "          Dropout-90          [-1, 160, 16, 16]               0\n",
            "      BatchNorm2d-91          [-1, 160, 16, 16]             320\n",
            "           Conv2d-92          [-1, 128, 16, 16]         184,448\n",
            "           Conv2d-93          [-1, 672, 16, 16]         968,352\n",
            "           Conv2d-94           [-1, 32, 16, 16]           1,056\n",
            " augmented_conv2d-95          [-1, 160, 16, 16]               0\n",
            "       wide_basic-96          [-1, 160, 16, 16]               0\n",
            "      BatchNorm2d-97          [-1, 160, 16, 16]             320\n",
            "           Conv2d-98          [-1, 128, 16, 16]         184,448\n",
            "           Conv2d-99          [-1, 672, 16, 16]         968,352\n",
            "          Conv2d-100           [-1, 32, 16, 16]           1,056\n",
            "augmented_conv2d-101          [-1, 160, 16, 16]               0\n",
            "         Dropout-102          [-1, 160, 16, 16]               0\n",
            "     BatchNorm2d-103          [-1, 160, 16, 16]             320\n",
            "          Conv2d-104          [-1, 128, 16, 16]         184,448\n",
            "          Conv2d-105          [-1, 672, 16, 16]         968,352\n",
            "          Conv2d-106           [-1, 32, 16, 16]           1,056\n",
            "augmented_conv2d-107          [-1, 160, 16, 16]               0\n",
            "      wide_basic-108          [-1, 160, 16, 16]               0\n",
            "     BatchNorm2d-109          [-1, 160, 16, 16]             320\n",
            "          Conv2d-110          [-1, 256, 16, 16]         368,896\n",
            "          Conv2d-111         [-1, 1344, 16, 16]       1,936,704\n",
            "          Conv2d-112           [-1, 64, 16, 16]           4,160\n",
            "augmented_conv2d-113          [-1, 320, 16, 16]               0\n",
            "         Dropout-114          [-1, 320, 16, 16]               0\n",
            "     BatchNorm2d-115          [-1, 320, 16, 16]             640\n",
            "          Conv2d-116            [-1, 256, 8, 8]         737,536\n",
            "          Conv2d-117           [-1, 1344, 8, 8]       3,872,064\n",
            "          Conv2d-118             [-1, 64, 8, 8]           4,160\n",
            "augmented_conv2d-119            [-1, 320, 8, 8]               0\n",
            "          Conv2d-120            [-1, 256, 8, 8]         368,896\n",
            "          Conv2d-121           [-1, 1344, 8, 8]       1,936,704\n",
            "          Conv2d-122             [-1, 64, 8, 8]           4,160\n",
            "augmented_conv2d-123            [-1, 320, 8, 8]               0\n",
            "      wide_basic-124            [-1, 320, 8, 8]               0\n",
            "     BatchNorm2d-125            [-1, 320, 8, 8]             640\n",
            "          Conv2d-126            [-1, 256, 8, 8]         737,536\n",
            "          Conv2d-127           [-1, 1344, 8, 8]       3,872,064\n",
            "          Conv2d-128             [-1, 64, 8, 8]           4,160\n",
            "augmented_conv2d-129            [-1, 320, 8, 8]               0\n",
            "         Dropout-130            [-1, 320, 8, 8]               0\n",
            "     BatchNorm2d-131            [-1, 320, 8, 8]             640\n",
            "          Conv2d-132            [-1, 256, 8, 8]         737,536\n",
            "          Conv2d-133           [-1, 1344, 8, 8]       3,872,064\n",
            "          Conv2d-134             [-1, 64, 8, 8]           4,160\n",
            "augmented_conv2d-135            [-1, 320, 8, 8]               0\n",
            "      wide_basic-136            [-1, 320, 8, 8]               0\n",
            "     BatchNorm2d-137            [-1, 320, 8, 8]             640\n",
            "          Conv2d-138            [-1, 256, 8, 8]         737,536\n",
            "          Conv2d-139           [-1, 1344, 8, 8]       3,872,064\n",
            "          Conv2d-140             [-1, 64, 8, 8]           4,160\n",
            "augmented_conv2d-141            [-1, 320, 8, 8]               0\n",
            "         Dropout-142            [-1, 320, 8, 8]               0\n",
            "     BatchNorm2d-143            [-1, 320, 8, 8]             640\n",
            "          Conv2d-144            [-1, 256, 8, 8]         737,536\n",
            "          Conv2d-145           [-1, 1344, 8, 8]       3,872,064\n",
            "          Conv2d-146             [-1, 64, 8, 8]           4,160\n",
            "augmented_conv2d-147            [-1, 320, 8, 8]               0\n",
            "      wide_basic-148            [-1, 320, 8, 8]               0\n",
            "     BatchNorm2d-149            [-1, 320, 8, 8]             640\n",
            "          Conv2d-150            [-1, 256, 8, 8]         737,536\n",
            "          Conv2d-151           [-1, 1344, 8, 8]       3,872,064\n",
            "          Conv2d-152             [-1, 64, 8, 8]           4,160\n",
            "augmented_conv2d-153            [-1, 320, 8, 8]               0\n",
            "         Dropout-154            [-1, 320, 8, 8]               0\n",
            "     BatchNorm2d-155            [-1, 320, 8, 8]             640\n",
            "          Conv2d-156            [-1, 256, 8, 8]         737,536\n",
            "          Conv2d-157           [-1, 1344, 8, 8]       3,872,064\n",
            "          Conv2d-158             [-1, 64, 8, 8]           4,160\n",
            "augmented_conv2d-159            [-1, 320, 8, 8]               0\n",
            "      wide_basic-160            [-1, 320, 8, 8]               0\n",
            "     BatchNorm2d-161            [-1, 320, 8, 8]             640\n",
            "AdaptiveAvgPool2d-162            [-1, 320, 1, 1]               0\n",
            "AdaptiveMaxPool2d-163            [-1, 320, 1, 1]               0\n",
            "          Linear-164                   [-1, 10]           6,410\n",
            "================================================================\n",
            "Total params: 48,332,822\n",
            "Trainable params: 48,332,822\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 91.19\n",
            "Params size (MB): 184.38\n",
            "Estimated Total Size (MB): 275.58\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}