{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SENet_v2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/maxmatical/pytorch-projects/blob/master/SENet_v2.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "PxqP7my30faV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "97f89a3b-bddc-49ef-f029-c896ada7842b"
      },
      "cell_type": "code",
      "source": [
        "# install pytorch 0.4.1 with gpu\n",
        "\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x574e8000 @  0x7fbe5fdad1c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s-wxtSvV0Znv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "38f59fb9-3633-41a4-96b4-72b948551b5e"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "print('Torch', torch.__version__, 'CUDA', torch.version.cuda)\n",
        "print('Device:', torch.device('cuda:0'))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4.1\n",
            "True\n",
            "Torch 0.4.1 CUDA 8.0.61\n",
            "Device: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QPDIfPx20yxR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "57d781bf-46cc-43fd-9e37-0b52d3468917"
      },
      "cell_type": "code",
      "source": [
        "  \n",
        "# install required packagles\n",
        "!pip install torchsummary"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchsummary\n",
            "  Downloading https://files.pythonhosted.org/packages/2a/61/21b44bb29aedb820fec4716a102e802397f0c21512764a9d98206c17417d/torchsummary-1.4-py3-none-any.whl\n",
            "Installing collected packages: torchsummary\n",
            "Successfully installed torchsummary-1.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OP2vMQnp0RJ5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "#import ipdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cXlUnQyx0RJ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FKhQnFHU0RKA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "                                        \n",
        "                                        \n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
        "                                         shuffle=False, num_workers=2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tFyN_PLd0RKS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, n_channels, reduction_ratio = 16):\n",
        "      super(SEBlock, self).__init__()\n",
        "      self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "      self.fc1 = nn.Linear(n_channels, n_channels//reduction_ratio)\n",
        "      self.relu = nn.LeakyReLU(inplace = True)\n",
        "      self.fc2 = nn.Linear(n_channels//reduction_ratio, n_channels)\n",
        "      self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input):\n",
        "      b, c, _, _ = input.size()\n",
        "      out = self.avg_pool(input).view(b,c)\n",
        "      out = self.relu(self.fc1(out))\n",
        "      out = self.sigmoid(self.fc2(out))\n",
        "      out = out.view(b,c,1,1)\n",
        "      out = out*input\n",
        "\n",
        "      return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2z290jxe72KC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class preact_se_res_block(nn.Module):\n",
        "    expansion = 1 # used in downsampling\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None, reduction_ratio = 16):\n",
        "        super(preact_se_res_block, self).__init__()\n",
        "        self.bn0 = nn.BatchNorm2d(in_channels)\n",
        "        # conv1 has defined stride\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride = stride, padding = 1, bias=False)  # stride for downsamplings\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding = 1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample \n",
        "        self.reduction_ratio = reduction_ratio\n",
        "        self.stride = stride\n",
        "        self.se = SEBlock(out_channels*self.expansion, reduction_ratio)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        # 1st conv layer (with downsampling)\n",
        "        out = self.bn0(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv1(out)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        # 2nd conv layer\n",
        "        out = self.conv2(out)\n",
        "        #out = self.bn2(out)\n",
        "        out = self.se(out)\n",
        "\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        #out = self.relu(out)\n",
        "        return out\n",
        "        \n",
        "\n",
        "                        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hha121eRIll7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# basic block (no preact)\n",
        "class SE_Res_Block_Basic(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None, reduction_ratio = 16):\n",
        "        super(SE_Res_Block_Basic, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False) \n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False) \n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.se = SEBlock(out_channels, reduction_ratio)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.se(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4E0NQY3T0RKV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# using pre-activation res blocks\n",
        "class SE_Res_Block_bottleneck(nn.Module): \n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None, reduction_ratio = 16):\n",
        "      super(SE_Res_Block_bottleneck, self).__init__()\n",
        "      self.conv1 = nn.Conv2d(in_channels, out_channels, 1, bias=False) \n",
        "      self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "      self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride,\n",
        "                             padding=1, bias=False)      \n",
        "      self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "      self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, 1, bias=False) \n",
        "      self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "      self.relu = nn.ReLU(inplace=True)\n",
        "      self.downsample = downsample \n",
        "      self.stride = stride \n",
        "      self.reduction_ratio = reduction_ratio\n",
        "      self.se = SEBlock(out_channels * self.expansion, reduction_ratio)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        #1x1 block\n",
        "        out = self.bn1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv1(out)\n",
        "        # downsampling block (3x3 conv)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        # expand block (1x1 conv)\n",
        "        out = self.bn3(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv3(out)\n",
        "        # SEblock\n",
        "        out = self.se(out)\n",
        "\n",
        "        # adjust for dimension mismatch\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "\n",
        "\n",
        "        return out\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BJVdGshL0RKX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# SENet\n",
        "n_classes = 10 \n",
        "\n",
        "class preact_SENet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes = n_classes, reduction_ratio = 16): # layer is a list\n",
        "        super(preact_SENet, self).__init__()\n",
        "        #initial conv layer\n",
        "        self.conv1 = nn.Conv2d(3, 16, 7, stride=1, padding = 3, bias = False) # initial conv layer similar to resnet\n",
        "        self.in_channels = 16 # match outchannel for conv1\n",
        "        self.batchnorm1 = nn.BatchNorm2d(16) # match outchannel for conv1\n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        ################\n",
        "        self.layer1 = self.make_layer(block, 16, layers[0], reduction_ratio = reduction_ratio)\n",
        "        self.layer2 = self.make_layer(block, 32, layers[1], stride = 2, reduction_ratio = reduction_ratio)\n",
        "        self.layer3 = self.make_layer(block, 64, layers[2], stride = 2, reduction_ratio = reduction_ratio)\n",
        "        self.layer4 = self.make_layer(block, 128, layers[3], stride = 2, reduction_ratio = reduction_ratio)\n",
        "        ################\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.maxpool = nn.AdaptiveMaxPool2d(1) \n",
        "        self.linear = nn.Linear(128*block.expansion, num_classes) # in_features = out_channel from last layer * expansion\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "    \n",
        "    \n",
        "    #######################\n",
        "    # make layers\n",
        "    #######################\n",
        "    \n",
        "    def make_layer(self, block, out_channels, blocks, reduction_ratio, stride=1):\n",
        "        # block = residual_block\n",
        "        # out_channel = output dimension of the block\n",
        "        # blocks = number of residual_block to use\n",
        "        # stride = stride length\n",
        "\n",
        "        downsample = None\n",
        "\n",
        "        # if dimesions don't match up\n",
        "        if (stride != 1) or (self.in_channels != out_channels * block.expansion):\n",
        "            downsample = nn.Sequential(\n",
        "            nn.Conv2d(self.in_channels, out_channels*block.expansion, kernel_size=1, stride = stride, bias = False),\n",
        "            nn.BatchNorm2d(out_channels*block.expansion))\n",
        "\n",
        "        # creating multiple layers of resblocks\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, downsample,  reduction_ratio))\n",
        "        self.in_channels = out_channels*block.expansion\n",
        "\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # initial conv layer to improve starting point\n",
        "        out = self.conv1(x)\n",
        "        out = self.batchnorm1(out)\n",
        "        out = self.relu(out)\n",
        "        #out = self.maxpool(out)\n",
        "        # resblocks\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avg_pool(out) # adaptive avg pooling to get (-1, out_channel(last layer), 1, 1)\n",
        "        out = out.view(out.size(0), -1) # flatten\n",
        "        out = self.linear(out) # output layer\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MwTcMnPm0RKa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "44a6ff88-8769-437d-d446-d695bd09262c"
      },
      "cell_type": "code",
      "source": [
        "#net1 = preact_SENet(SEBottleneck, layers = [2,2,2,2])\n",
        "net2 = preact_SENet(preact_se_res_block, layers = [2,2,2,2])\n",
        "net3 = preact_SENet(SE_Res_Block_bottleneck, layers = [2,2,2,2])\n",
        "# enable GPU\n",
        "use_cuda = True\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    #net1.cuda()\n",
        "    net2.cuda()\n",
        "    net3.cuda()\n",
        "    \n",
        "# check if models are on cuda\n",
        "#print(next(net1.parameters()).is_cuda)\n",
        "print(next(net2.parameters()).is_cuda)\n",
        "print(next(net3.parameters()).is_cuda)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PWJMuDtsPw4N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2167
        },
        "outputId": "4de2341b-0f09-44ae-ebb5-ff05fcd3d7aa"
      },
      "cell_type": "code",
      "source": [
        "summary(net2, (3, 32, 32))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 32, 32]           2,352\n",
            "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
            "              ReLU-3           [-1, 16, 32, 32]               0\n",
            "       BatchNorm2d-4           [-1, 16, 32, 32]              32\n",
            "              ReLU-5           [-1, 16, 32, 32]               0\n",
            "            Conv2d-6           [-1, 16, 32, 32]           2,304\n",
            "       BatchNorm2d-7           [-1, 16, 32, 32]              32\n",
            "              ReLU-8           [-1, 16, 32, 32]               0\n",
            "            Conv2d-9           [-1, 16, 32, 32]           2,304\n",
            "AdaptiveAvgPool2d-10             [-1, 16, 1, 1]               0\n",
            "           Linear-11                    [-1, 1]              17\n",
            "        LeakyReLU-12                    [-1, 1]               0\n",
            "           Linear-13                   [-1, 16]              32\n",
            "          Sigmoid-14                   [-1, 16]               0\n",
            "          SEBlock-15           [-1, 16, 32, 32]               0\n",
            "preact_se_res_block-16           [-1, 16, 32, 32]               0\n",
            "      BatchNorm2d-17           [-1, 16, 32, 32]              32\n",
            "             ReLU-18           [-1, 16, 32, 32]               0\n",
            "           Conv2d-19           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-20           [-1, 16, 32, 32]              32\n",
            "             ReLU-21           [-1, 16, 32, 32]               0\n",
            "           Conv2d-22           [-1, 16, 32, 32]           2,304\n",
            "AdaptiveAvgPool2d-23             [-1, 16, 1, 1]               0\n",
            "           Linear-24                    [-1, 1]              17\n",
            "        LeakyReLU-25                    [-1, 1]               0\n",
            "           Linear-26                   [-1, 16]              32\n",
            "          Sigmoid-27                   [-1, 16]               0\n",
            "          SEBlock-28           [-1, 16, 32, 32]               0\n",
            "preact_se_res_block-29           [-1, 16, 32, 32]               0\n",
            "      BatchNorm2d-30           [-1, 16, 32, 32]              32\n",
            "             ReLU-31           [-1, 16, 32, 32]               0\n",
            "           Conv2d-32           [-1, 32, 16, 16]           4,608\n",
            "      BatchNorm2d-33           [-1, 32, 16, 16]              64\n",
            "             ReLU-34           [-1, 32, 16, 16]               0\n",
            "           Conv2d-35           [-1, 32, 16, 16]           9,216\n",
            "AdaptiveAvgPool2d-36             [-1, 32, 1, 1]               0\n",
            "           Linear-37                    [-1, 2]              66\n",
            "        LeakyReLU-38                    [-1, 2]               0\n",
            "           Linear-39                   [-1, 32]              96\n",
            "          Sigmoid-40                   [-1, 32]               0\n",
            "          SEBlock-41           [-1, 32, 16, 16]               0\n",
            "           Conv2d-42           [-1, 32, 16, 16]             512\n",
            "      BatchNorm2d-43           [-1, 32, 16, 16]              64\n",
            "preact_se_res_block-44           [-1, 32, 16, 16]               0\n",
            "      BatchNorm2d-45           [-1, 32, 16, 16]              64\n",
            "             ReLU-46           [-1, 32, 16, 16]               0\n",
            "           Conv2d-47           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-48           [-1, 32, 16, 16]              64\n",
            "             ReLU-49           [-1, 32, 16, 16]               0\n",
            "           Conv2d-50           [-1, 32, 16, 16]           9,216\n",
            "AdaptiveAvgPool2d-51             [-1, 32, 1, 1]               0\n",
            "           Linear-52                    [-1, 2]              66\n",
            "        LeakyReLU-53                    [-1, 2]               0\n",
            "           Linear-54                   [-1, 32]              96\n",
            "          Sigmoid-55                   [-1, 32]               0\n",
            "          SEBlock-56           [-1, 32, 16, 16]               0\n",
            "preact_se_res_block-57           [-1, 32, 16, 16]               0\n",
            "      BatchNorm2d-58           [-1, 32, 16, 16]              64\n",
            "             ReLU-59           [-1, 32, 16, 16]               0\n",
            "           Conv2d-60             [-1, 64, 8, 8]          18,432\n",
            "      BatchNorm2d-61             [-1, 64, 8, 8]             128\n",
            "             ReLU-62             [-1, 64, 8, 8]               0\n",
            "           Conv2d-63             [-1, 64, 8, 8]          36,864\n",
            "AdaptiveAvgPool2d-64             [-1, 64, 1, 1]               0\n",
            "           Linear-65                    [-1, 4]             260\n",
            "        LeakyReLU-66                    [-1, 4]               0\n",
            "           Linear-67                   [-1, 64]             320\n",
            "          Sigmoid-68                   [-1, 64]               0\n",
            "          SEBlock-69             [-1, 64, 8, 8]               0\n",
            "           Conv2d-70             [-1, 64, 8, 8]           2,048\n",
            "      BatchNorm2d-71             [-1, 64, 8, 8]             128\n",
            "preact_se_res_block-72             [-1, 64, 8, 8]               0\n",
            "      BatchNorm2d-73             [-1, 64, 8, 8]             128\n",
            "             ReLU-74             [-1, 64, 8, 8]               0\n",
            "           Conv2d-75             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-76             [-1, 64, 8, 8]             128\n",
            "             ReLU-77             [-1, 64, 8, 8]               0\n",
            "           Conv2d-78             [-1, 64, 8, 8]          36,864\n",
            "AdaptiveAvgPool2d-79             [-1, 64, 1, 1]               0\n",
            "           Linear-80                    [-1, 4]             260\n",
            "        LeakyReLU-81                    [-1, 4]               0\n",
            "           Linear-82                   [-1, 64]             320\n",
            "          Sigmoid-83                   [-1, 64]               0\n",
            "          SEBlock-84             [-1, 64, 8, 8]               0\n",
            "preact_se_res_block-85             [-1, 64, 8, 8]               0\n",
            "      BatchNorm2d-86             [-1, 64, 8, 8]             128\n",
            "             ReLU-87             [-1, 64, 8, 8]               0\n",
            "           Conv2d-88            [-1, 128, 4, 4]          73,728\n",
            "      BatchNorm2d-89            [-1, 128, 4, 4]             256\n",
            "             ReLU-90            [-1, 128, 4, 4]               0\n",
            "           Conv2d-91            [-1, 128, 4, 4]         147,456\n",
            "AdaptiveAvgPool2d-92            [-1, 128, 1, 1]               0\n",
            "           Linear-93                    [-1, 8]           1,032\n",
            "        LeakyReLU-94                    [-1, 8]               0\n",
            "           Linear-95                  [-1, 128]           1,152\n",
            "          Sigmoid-96                  [-1, 128]               0\n",
            "          SEBlock-97            [-1, 128, 4, 4]               0\n",
            "           Conv2d-98            [-1, 128, 4, 4]           8,192\n",
            "      BatchNorm2d-99            [-1, 128, 4, 4]             256\n",
            "preact_se_res_block-100            [-1, 128, 4, 4]               0\n",
            "     BatchNorm2d-101            [-1, 128, 4, 4]             256\n",
            "            ReLU-102            [-1, 128, 4, 4]               0\n",
            "          Conv2d-103            [-1, 128, 4, 4]         147,456\n",
            "     BatchNorm2d-104            [-1, 128, 4, 4]             256\n",
            "            ReLU-105            [-1, 128, 4, 4]               0\n",
            "          Conv2d-106            [-1, 128, 4, 4]         147,456\n",
            "AdaptiveAvgPool2d-107            [-1, 128, 1, 1]               0\n",
            "          Linear-108                    [-1, 8]           1,032\n",
            "       LeakyReLU-109                    [-1, 8]               0\n",
            "          Linear-110                  [-1, 128]           1,152\n",
            "         Sigmoid-111                  [-1, 128]               0\n",
            "         SEBlock-112            [-1, 128, 4, 4]               0\n",
            "preact_se_res_block-113            [-1, 128, 4, 4]               0\n",
            "AdaptiveAvgPool2d-114            [-1, 128, 1, 1]               0\n",
            "          Linear-115                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 709,112\n",
            "Trainable params: 709,112\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 4.57\n",
            "Params size (MB): 2.71\n",
            "Estimated Total Size (MB): 7.29\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yO10I2PP0RKc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2604
        },
        "outputId": "9daf7351-5acf-463a-859c-4838d60729eb"
      },
      "cell_type": "code",
      "source": [
        "# summary\n",
        "summary(net3, (3, 32, 32))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 32, 32]           2,352\n",
            "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
            "              ReLU-3           [-1, 16, 32, 32]               0\n",
            "       BatchNorm2d-4           [-1, 16, 32, 32]              32\n",
            "              ReLU-5           [-1, 16, 32, 32]               0\n",
            "            Conv2d-6           [-1, 16, 32, 32]             256\n",
            "       BatchNorm2d-7           [-1, 16, 32, 32]              32\n",
            "              ReLU-8           [-1, 16, 32, 32]               0\n",
            "            Conv2d-9           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-10           [-1, 16, 32, 32]              32\n",
            "             ReLU-11           [-1, 16, 32, 32]               0\n",
            "           Conv2d-12           [-1, 64, 32, 32]           1,024\n",
            "AdaptiveAvgPool2d-13             [-1, 64, 1, 1]               0\n",
            "           Linear-14                    [-1, 4]             260\n",
            "        LeakyReLU-15                    [-1, 4]               0\n",
            "           Linear-16                   [-1, 64]             320\n",
            "          Sigmoid-17                   [-1, 64]               0\n",
            "          SEBlock-18           [-1, 64, 32, 32]               0\n",
            "           Conv2d-19           [-1, 64, 32, 32]           1,024\n",
            "      BatchNorm2d-20           [-1, 64, 32, 32]             128\n",
            "SE_Res_Block_bottleneck-21           [-1, 64, 32, 32]               0\n",
            "      BatchNorm2d-22           [-1, 64, 32, 32]             128\n",
            "             ReLU-23           [-1, 64, 32, 32]               0\n",
            "           Conv2d-24           [-1, 16, 32, 32]           1,024\n",
            "      BatchNorm2d-25           [-1, 16, 32, 32]              32\n",
            "             ReLU-26           [-1, 16, 32, 32]               0\n",
            "           Conv2d-27           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-28           [-1, 16, 32, 32]              32\n",
            "             ReLU-29           [-1, 16, 32, 32]               0\n",
            "           Conv2d-30           [-1, 64, 32, 32]           1,024\n",
            "AdaptiveAvgPool2d-31             [-1, 64, 1, 1]               0\n",
            "           Linear-32                    [-1, 4]             260\n",
            "        LeakyReLU-33                    [-1, 4]               0\n",
            "           Linear-34                   [-1, 64]             320\n",
            "          Sigmoid-35                   [-1, 64]               0\n",
            "          SEBlock-36           [-1, 64, 32, 32]               0\n",
            "SE_Res_Block_bottleneck-37           [-1, 64, 32, 32]               0\n",
            "      BatchNorm2d-38           [-1, 64, 32, 32]             128\n",
            "             ReLU-39           [-1, 64, 32, 32]               0\n",
            "           Conv2d-40           [-1, 32, 32, 32]           2,048\n",
            "      BatchNorm2d-41           [-1, 32, 32, 32]              64\n",
            "             ReLU-42           [-1, 32, 32, 32]               0\n",
            "           Conv2d-43           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-44           [-1, 32, 16, 16]              64\n",
            "             ReLU-45           [-1, 32, 16, 16]               0\n",
            "           Conv2d-46          [-1, 128, 16, 16]           4,096\n",
            "AdaptiveAvgPool2d-47            [-1, 128, 1, 1]               0\n",
            "           Linear-48                    [-1, 8]           1,032\n",
            "        LeakyReLU-49                    [-1, 8]               0\n",
            "           Linear-50                  [-1, 128]           1,152\n",
            "          Sigmoid-51                  [-1, 128]               0\n",
            "          SEBlock-52          [-1, 128, 16, 16]               0\n",
            "           Conv2d-53          [-1, 128, 16, 16]           8,192\n",
            "      BatchNorm2d-54          [-1, 128, 16, 16]             256\n",
            "SE_Res_Block_bottleneck-55          [-1, 128, 16, 16]               0\n",
            "      BatchNorm2d-56          [-1, 128, 16, 16]             256\n",
            "             ReLU-57          [-1, 128, 16, 16]               0\n",
            "           Conv2d-58           [-1, 32, 16, 16]           4,096\n",
            "      BatchNorm2d-59           [-1, 32, 16, 16]              64\n",
            "             ReLU-60           [-1, 32, 16, 16]               0\n",
            "           Conv2d-61           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-62           [-1, 32, 16, 16]              64\n",
            "             ReLU-63           [-1, 32, 16, 16]               0\n",
            "           Conv2d-64          [-1, 128, 16, 16]           4,096\n",
            "AdaptiveAvgPool2d-65            [-1, 128, 1, 1]               0\n",
            "           Linear-66                    [-1, 8]           1,032\n",
            "        LeakyReLU-67                    [-1, 8]               0\n",
            "           Linear-68                  [-1, 128]           1,152\n",
            "          Sigmoid-69                  [-1, 128]               0\n",
            "          SEBlock-70          [-1, 128, 16, 16]               0\n",
            "SE_Res_Block_bottleneck-71          [-1, 128, 16, 16]               0\n",
            "      BatchNorm2d-72          [-1, 128, 16, 16]             256\n",
            "             ReLU-73          [-1, 128, 16, 16]               0\n",
            "           Conv2d-74           [-1, 64, 16, 16]           8,192\n",
            "      BatchNorm2d-75           [-1, 64, 16, 16]             128\n",
            "             ReLU-76           [-1, 64, 16, 16]               0\n",
            "           Conv2d-77             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-78             [-1, 64, 8, 8]             128\n",
            "             ReLU-79             [-1, 64, 8, 8]               0\n",
            "           Conv2d-80            [-1, 256, 8, 8]          16,384\n",
            "AdaptiveAvgPool2d-81            [-1, 256, 1, 1]               0\n",
            "           Linear-82                   [-1, 16]           4,112\n",
            "        LeakyReLU-83                   [-1, 16]               0\n",
            "           Linear-84                  [-1, 256]           4,352\n",
            "          Sigmoid-85                  [-1, 256]               0\n",
            "          SEBlock-86            [-1, 256, 8, 8]               0\n",
            "           Conv2d-87            [-1, 256, 8, 8]          32,768\n",
            "      BatchNorm2d-88            [-1, 256, 8, 8]             512\n",
            "SE_Res_Block_bottleneck-89            [-1, 256, 8, 8]               0\n",
            "      BatchNorm2d-90            [-1, 256, 8, 8]             512\n",
            "             ReLU-91            [-1, 256, 8, 8]               0\n",
            "           Conv2d-92             [-1, 64, 8, 8]          16,384\n",
            "      BatchNorm2d-93             [-1, 64, 8, 8]             128\n",
            "             ReLU-94             [-1, 64, 8, 8]               0\n",
            "           Conv2d-95             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-96             [-1, 64, 8, 8]             128\n",
            "             ReLU-97             [-1, 64, 8, 8]               0\n",
            "           Conv2d-98            [-1, 256, 8, 8]          16,384\n",
            "AdaptiveAvgPool2d-99            [-1, 256, 1, 1]               0\n",
            "          Linear-100                   [-1, 16]           4,112\n",
            "       LeakyReLU-101                   [-1, 16]               0\n",
            "          Linear-102                  [-1, 256]           4,352\n",
            "         Sigmoid-103                  [-1, 256]               0\n",
            "         SEBlock-104            [-1, 256, 8, 8]               0\n",
            "SE_Res_Block_bottleneck-105            [-1, 256, 8, 8]               0\n",
            "     BatchNorm2d-106            [-1, 256, 8, 8]             512\n",
            "            ReLU-107            [-1, 256, 8, 8]               0\n",
            "          Conv2d-108            [-1, 128, 8, 8]          32,768\n",
            "     BatchNorm2d-109            [-1, 128, 8, 8]             256\n",
            "            ReLU-110            [-1, 128, 8, 8]               0\n",
            "          Conv2d-111            [-1, 128, 4, 4]         147,456\n",
            "     BatchNorm2d-112            [-1, 128, 4, 4]             256\n",
            "            ReLU-113            [-1, 128, 4, 4]               0\n",
            "          Conv2d-114            [-1, 512, 4, 4]          65,536\n",
            "AdaptiveAvgPool2d-115            [-1, 512, 1, 1]               0\n",
            "          Linear-116                   [-1, 32]          16,416\n",
            "       LeakyReLU-117                   [-1, 32]               0\n",
            "          Linear-118                  [-1, 512]          16,896\n",
            "         Sigmoid-119                  [-1, 512]               0\n",
            "         SEBlock-120            [-1, 512, 4, 4]               0\n",
            "          Conv2d-121            [-1, 512, 4, 4]         131,072\n",
            "     BatchNorm2d-122            [-1, 512, 4, 4]           1,024\n",
            "SE_Res_Block_bottleneck-123            [-1, 512, 4, 4]               0\n",
            "     BatchNorm2d-124            [-1, 512, 4, 4]           1,024\n",
            "            ReLU-125            [-1, 512, 4, 4]               0\n",
            "          Conv2d-126            [-1, 128, 4, 4]          65,536\n",
            "     BatchNorm2d-127            [-1, 128, 4, 4]             256\n",
            "            ReLU-128            [-1, 128, 4, 4]               0\n",
            "          Conv2d-129            [-1, 128, 4, 4]         147,456\n",
            "     BatchNorm2d-130            [-1, 128, 4, 4]             256\n",
            "            ReLU-131            [-1, 128, 4, 4]               0\n",
            "          Conv2d-132            [-1, 512, 4, 4]          65,536\n",
            "AdaptiveAvgPool2d-133            [-1, 512, 1, 1]               0\n",
            "          Linear-134                   [-1, 32]          16,416\n",
            "       LeakyReLU-135                   [-1, 32]               0\n",
            "          Linear-136                  [-1, 512]          16,896\n",
            "         Sigmoid-137                  [-1, 512]               0\n",
            "         SEBlock-138            [-1, 512, 4, 4]               0\n",
            "SE_Res_Block_bottleneck-139            [-1, 512, 4, 4]               0\n",
            "AdaptiveAvgPool2d-140            [-1, 512, 1, 1]               0\n",
            "          Linear-141                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 972,402\n",
            "Trainable params: 972,402\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 15.60\n",
            "Params size (MB): 3.71\n",
            "Estimated Total Size (MB): 19.32\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NUgoOUVH0RKr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# weight initialization\n",
        "# new init weight        \n",
        "def init_weight(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight) #he initialize, can use xavier instead\n",
        "        #nn.init.constant_(m.bias, 0.001) # optional bias\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_normal_(m.weight) #he initialize, can use xavier instead\n",
        "        #nn.init.constant_(m.bias, 0.001) # optional bias\n",
        "    elif type(m) == nn.BatchNorm2d:\n",
        "        torch.nn.init.constant_(m.weight, 1)\n",
        "        torch.nn.init.constant_(m.bias, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J8TpiUWt0RKt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# apply initializers\n",
        "net.apply(init_weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OsjqWehH0RKw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define loss and optimizer\n",
        "import torch.optim as optim\n",
        "learning_rate = 0.01\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr = learning_rate, momentum=0.9, nesterov= True, weight_decay= 0.01)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qpybaz9_0RKz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# LR scheduler\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "\n",
        "class CosineAnnealingLR_with_Restart(_LRScheduler):\n",
        "    \"\"\"Set the learning rate of each parameter group using a cosine annealing\n",
        "    schedule, where :math:`\\eta_{max}` is set to the initial lr and\n",
        "    :math:`T_{cur}` is the number of epochs since the last restart in SGDR:\n",
        "\n",
        "    .. math::\n",
        "\n",
        "        \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 +\n",
        "        \\cos(\\frac{T_{cur}}{T_{max}}\\pi))\n",
        "\n",
        "    When last_epoch=-1, sets initial lr as lr.\n",
        "\n",
        "    It has been proposed in\n",
        "    `SGDR: Stochastic Gradient Descent with Warm Restarts`_. The original pytorch\n",
        "    implementation only implements the cosine annealing part of SGDR,\n",
        "    I added my own implementation of the restarts part.\n",
        "    \n",
        "    Args:\n",
        "        optimizer (Optimizer): Wrapped optimizer.\n",
        "        T_max (int): Maximum number of iterations. (LENGTH OF 1 CYCLE)\n",
        "        T_mult (float): Increase T_max by a factor of T_mult\n",
        "        eta_min (float): Minimum learning rate. Default: 0.\n",
        "        last_epoch (int): The index of last epoch. Default: -1.\n",
        "        model (pytorch model): The model to save.\n",
        "        out_dir (str): Directory to save snapshots\n",
        "        take_snapshot (bool): Whether to save snapshots at every restart\n",
        "\n",
        "    .. _SGDR\\: Stochastic Gradient Descent with Warm Restarts:\n",
        "        https://arxiv.org/abs/1608.03983\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, optimizer, T_max, T_mult, model, out_dir, take_snapshot, eta_min=0, last_epoch=-1):\n",
        "        self.T_max = T_max\n",
        "        self.T_mult = T_mult\n",
        "        self.Te = self.T_max\n",
        "        self.eta_min = eta_min\n",
        "        self.current_epoch = last_epoch\n",
        "        \n",
        "        self.model = model\n",
        "        self.out_dir = out_dir\n",
        "        self.take_snapshot = take_snapshot\n",
        "        \n",
        "        self.lr_history = []\n",
        "        \n",
        "        super(CosineAnnealingLR_with_Restart, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        new_lrs = [self.eta_min + (base_lr - self.eta_min) *\n",
        "                (1 + math.cos(math.pi * self.current_epoch / self.Te)) / 2\n",
        "\n",
        "                for base_lr in self.base_lrs]\n",
        "        \n",
        "        self.lr_history.append(new_lrs)\n",
        "        return new_lrs\n",
        "    \n",
        "    def step(self, epoch=None):\n",
        "        if epoch is None:\n",
        "        \n",
        "            epoch = self.last_epoch + 1\n",
        "        self.last_epoch = epoch\n",
        "        self.current_epoch += 1\n",
        "        \n",
        "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
        "            param_group['lr'] = lr\n",
        "        \n",
        "        ## restart\n",
        "        if self.current_epoch == self.Te:\n",
        "            print(\"restart at epoch {:03d}\".format(self.last_epoch + 1))\n",
        "            \n",
        "            if self.take_snapshot:\n",
        "                torch.save({\n",
        "                    'epoch': self.T_max,\n",
        "                    'state_dict': self.model.state_dict()\n",
        "                }, self.out_dir + \"/\" + 'snapshot_e_{:03d}.pth.tar'.format(self.T_max))\n",
        "            \n",
        "            ## reset epochs since the last reset\n",
        "            self.current_epoch = 0\n",
        "            \n",
        "            ## reset the next goal\n",
        "            self.Te = int(self.Te * self.T_mult)\n",
        "            self.T_max = self.T_max + self.Te"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vTqqSPzw0RK2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# T_max = how many Epochs before restarting learning rate\n",
        "# T_mult = increase cycle length after restart \n",
        "\n",
        "# try:\n",
        "# 1st training cycle: T_max = 3, T_mult = 1 for 3 cycles (9 epochs)\n",
        "# 2nd training cycle: T_max = 3, T_mult = 2 for 3 cycles (21 epochs)\n",
        "\n",
        "scheduler = CosineAnnealingLR_with_Restart(optimizer, T_max=2, T_mult=2, model = net,  out_dir='blank', take_snapshot=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2njsc51I0RK7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# modified model training to keep track of train/val loss\n",
        "n_epochs = 6\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    scheduler.step()\n",
        "    running_loss = 0.0\n",
        "    total_train_loss = 0.0\n",
        "    for i, train_data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = train_data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print loss per n minibatches\n",
        "        running_loss += loss.item()\n",
        "        total_train_loss += loss.item()\n",
        "        if i % 500 == 499:    # print every 500 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 500))\n",
        "            running_loss = 0.0\n",
        "    \n",
        "    # keep track of loss in test dataset \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for test_data in testloader:\n",
        "            test_images, test_labels = test_data\n",
        "            test_outputs = net(test_images)\n",
        "            test_loss = criterion(test_outputs, test_labels)\n",
        "            total_test_loss += test_loss.item()\n",
        "            _, predicted = torch.max(test_outputs.data, 1)\n",
        "            total += test_labels.size(0)\n",
        "            correct += (predicted == test_labels).sum().item()\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    # for printing average loss every epoch\n",
        "    print(\"===> Epoch {} Complete: Train Avg. Loss: {:.4f}\".format(epoch+1, total_train_loss / len(trainloader)))\n",
        "    print(\"===> Epoch {} Complete: Test Avg. Loss: {:.4f}\".format(epoch+1, total_test_loss / len(testloader)))\n",
        "    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5MExUtlg0RK_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}