{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "xresnet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxmatical/pytorch-projects/blob/master/xresnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol7lXaYdj2T9",
        "colab_type": "text"
      },
      "source": [
        "#XResnet\n",
        "\n",
        "Implementation of xresnet from https://arxiv.org/abs/1812.01187 and https://github.com/fastai/fastai/blob/master/fastai/vision/models/xresnet.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTEtXA4bjmUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGUPmQ9Oj2J4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Flatten(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "    \n",
        "# weight initialization\n",
        "def init_cnn(m):\n",
        "    if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0)\n",
        "    if isinstance(m, (nn.Conv2d,nn.Linear)): nn.init.kaiming_normal_(m.weight)\n",
        "    for l in m.children(): init_cnn(l)\n",
        "\n",
        "# activation function\n",
        "act_fn = nn.ReLU(inplace=True)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn6TDvBnlNxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# default conv layer\n",
        "def conv(in_channels, out_channels, kernel_size = 3, stride = 1, bias = False):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size = kernel_size, stride = stride, padding = kernel_size//2, bias = bias)\n",
        "\n",
        "\n",
        "# conv + bn + act_fun\n",
        "def conv_layer(in_channels, out_channels, kernel_size=3, stride=1, zero_bn=False, act=True):\n",
        "    bn = nn.BatchNorm2d(out_channels)\n",
        "    nn.init.constant_(bn.weight, 0. if zero_bn else 1.)\n",
        "    layers = [conv(in_channels, out_channels, kernel_size, stride = stride), bn]\n",
        "    if act: \n",
        "        layers.append(act_fn)\n",
        "    return nn.Sequential(*layers)\n",
        "       \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYYjcl65QyvP",
        "colab_type": "text"
      },
      "source": [
        "In the Resblock, the last conv_layer is set to zero_bn \n",
        "\n",
        "This is because if zero_bn is true, the entire conv_layer is set to 0 since the bn sets the outputs to 0\n",
        "\n",
        "This lets the NN learn when a resblock is not needed, it learns to skip over the block if it doesn't contribute\n",
        "\n",
        "If it contributes, then it learns the weight of the bn so then the resblock actually has weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obQToJTioYyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# residual block\n",
        "\n",
        "def no_op(x): return x # no operations done if in_channels == out_channels\n",
        "\n",
        "\n",
        "class Resblock(nn.Module):\n",
        "    def __init__(self, expansion, in_channels, mid_channels, stride = 1):\n",
        "        super(Resblock, self).__init__()\n",
        "        out_channels, in_channels = mid_channels*expansion, in_channels*expansion\n",
        "        if expansion == 1:\n",
        "            layers = [conv_layer(in_channels, mid_channels, 3, stride = stride), #either stride 1 (if res block) or stride = 2 for downsampling blocks\n",
        "                      conv_layer(mid_channels, out_channels, 3, zero_bn = True, act = False)] # stride 1\n",
        "        else:\n",
        "            layers = [conv_layer(in_channels, mid_channels, 1),\n",
        "                      conv_layer(mid_channels, mid_channels, 3, stride = stride), #either stride 1 (if res block) or stride = 2 for downsampling blocks\n",
        "                      conv_layer(mid_channels, out_channels, 1, zero_bn = True, act = False)]\n",
        "            \n",
        "        self.convs = nn.Sequential(*layers)\n",
        "        \n",
        "        # if in_channels != out_channels, use a 1x1 conv to get the same channels, otherwise return x (no operations)\n",
        "        self.idconv = no_op if in_channels == out_channels else conv_layer(in_channels, out_channels, 1, act = False)\n",
        "        \n",
        "        self.pooling = no_op if stride == 1 else nn.AvgPool2d(2, ceil_mode=True)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x1 = self.convs(x) # convs operations\n",
        "        x2 = self.idconv(self.pooling(x)) # pooling and 1x1 conv layer operations\n",
        "        out = x1+x2\n",
        "        return act_fn(out)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTOJ5Z8iR-0F",
        "colab_type": "text"
      },
      "source": [
        "For XResNet. Replaces the 7x7 stride 2 stem with 3 3x3 convs (stride = 2 for first conv) in a row\n",
        "\n",
        "This is done beecause 3x3 convolutions are much less computationally expensive than a 7x7 (5.4 times more expensive)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueOjqfgYxapy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating XResNet\n",
        "class XResNet(nn.Sequential):\n",
        "    def __init__(self, expansion, layers, in_channels = 3, n_classes=1000):\n",
        "        \n",
        "        \"\"\"\n",
        "        layers = list of length 4. \n",
        "        layer[i] = how many resblocks in each of the 4 chunks of the network\n",
        "        expansion = what value to multiply the intermediate n_out of the convlayer by\n",
        "        \"\"\"\n",
        "        #stem\n",
        "        stem = []\n",
        "        stem_sizes = [in_channels, 32, 32, 64]\n",
        "        for i in range(3):\n",
        "            stem.append(conv_layer(stem_sizes[i], stem_sizes[i+1], stride = 2 if i==0 else 1))\n",
        "            \n",
        "        # creating the resblock layers\n",
        "        block_sizes = [64//expansion, 64, 128, 256, 512]\n",
        "        blocks = [self._make_layer(expansion, in_channels = block_sizes[i], out_channels = block_sizes[i+1], blocks = l, stride = 1 if i == 0 else 2) #1st stage has no downsampling\n",
        "                    for i, l in enumerate(layers)] #l in enumerate(layers) goes through layers in XResNet and sets the value of blocks based on layer[i]\n",
        "        \n",
        "        # creating network\n",
        "        super().__init__(*stem,\n",
        "                      nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "                      *blocks,\n",
        "                      nn.AdaptiveAvgPool2d(1), \n",
        "                      Flatten(),\n",
        "                      nn.Linear(block_sizes[-1]*expansion, n_classes)\n",
        "        )\n",
        "        \n",
        "        init_weight(self)\n",
        "        \n",
        "    def _make_layer(self, expansion, in_channels, out_channels, blocks, stride):\n",
        "        \"\"\"\n",
        "        blocks = int. -> number of blocks to create = layer[i]\n",
        "        \"\"\"\n",
        "        return nn.Sequential(\n",
        "            *[Resblock(expansion, in_channels if i == 0 else out_channels, out_channels, stride if i==0 else 1) # only stride 2 for the downsampling block(first block) and stride = 1 for residual blocks\n",
        "              for i in range(blocks)])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yDf8BNukFoc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xresnet_18 = XResNet(expansion = 1, layers = [2, 2, 2, 2])\n",
        "xresnet_34 = XResNet(expansion = 1, layers = [3, 4, 6, 3])\n",
        "xresnet_50 = XResNet(expansion = 4, layers = [3, 4, 6, 3])\n",
        "xresnet_101 = XResNet(expansion = 4, layers = [3,4,23,3])\n",
        "xresnet_152 = XResNet(expansion = 4, layers = [3,8,36,3])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRQQbHfllHix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}