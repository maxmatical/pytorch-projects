{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DenseNet-v3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/maxmatical/pytorch-projects/blob/master/DenseNet_v3.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "PxqP7my30faV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "027a4aff-e1bb-4e6e-99dd-022fb8c002d6"
      },
      "cell_type": "code",
      "source": [
        "# install pytorch 0.4.1 with gpu\n",
        "\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x567e0000 @  0x7f902def41c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s-wxtSvV0Znv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "1b3e4fa2-fdb9-4d30-dd92-9c64b2a3d6f7"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "print('Torch', torch.__version__, 'CUDA', torch.version.cuda)\n",
        "print('Device:', torch.device('cuda:0'))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4.1\n",
            "True\n",
            "Torch 0.4.1 CUDA 8.0.61\n",
            "Device: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QPDIfPx20yxR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "40286604-12c9-4c9e-bcee-68540dc7989b"
      },
      "cell_type": "code",
      "source": [
        "  \n",
        "# install required packagles\n",
        "!pip install torchsummary"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchsummary\n",
            "  Downloading https://files.pythonhosted.org/packages/2a/61/21b44bb29aedb820fec4716a102e802397f0c21512764a9d98206c17417d/torchsummary-1.4-py3-none-any.whl\n",
            "Installing collected packages: torchsummary\n",
            "Successfully installed torchsummary-1.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OP2vMQnp0RJ5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "#import ipdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cXlUnQyx0RJ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FKhQnFHU0RKA",
        "colab_type": "code",
        "colab": {},
        "outputId": "175ef69b-862e-447f-8a3a-eec4caaa34fe"
      },
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "                                        \n",
        "                                        \n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
        "                                         shuffle=False, num_workers=2)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tFyN_PLd0RKS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.2, stride = 1):\n",
        "        super(DenseBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        self.leaky_relu = nn.LeakyReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_channels, 4*out_channels, 1, stride = 1, padding=0, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(4*out_channels)\n",
        "        self.conv2 = nn.Conv2d(4*out_channels, out_channels, 3, stride = 1, padding=1, bias = False)\n",
        "        self.dropout_prob = dropout\n",
        "        self.stride = stride\n",
        "        \n",
        "    def forward(self, input):\n",
        "        out = self.conv1(self.leaky_relu(self.bn1(input)))\n",
        "        out = F.dropout(out, p=self.dropout_prob, inplace=False, training = self.training)\n",
        "        out = self.conv2(self.leaky_relu(self.bn2(out)))\n",
        "        out = F.dropout(out, p=self.dropout_prob, inplace=False, training = self.training)\n",
        "        out = torch.cat([out,input],1)\n",
        "        return out\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4E0NQY3T0RKV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TransitionBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.2):\n",
        "        super(TransitionBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        self.leaky_relu = nn.LeakyReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 1, stride = 1, padding=0, bias=False)\n",
        "        self.dropout_prob = dropout\n",
        "        self.avgpool = nn.AvgPool2d(2, stride = 2)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        out = self.conv1(self.leaky_relu(self.bn1(input)))\n",
        "        out = F.dropout(out, p=self.dropout_prob, inplace=False, training = self.training)\n",
        "        out = self.avgpool(out)\n",
        "        return out\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BJVdGshL0RKX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MwTcMnPm0RKa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yO10I2PP0RKc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DenseNet\n",
        "n_classes = 10 \n",
        "growth_rate = 12 # growth rate\n",
        "compression_rate = 0.5 # theta\n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, block, layers, dropout =0.2, num_classes = n_classes,k=growth_rate, theta = compression_rate): # layer is a list\n",
        "        super(DenseNet, self).__init__()\n",
        "        \n",
        "        # defining initial in_plane\n",
        "        in_channel = 2*k\n",
        "        \n",
        "        #self.layer = self.make_layer(block, in_channel, k, n_layers, dropout)\n",
        "\n",
        "        self.dropout = dropout\n",
        "        \n",
        "\n",
        "        #initial conv layers\n",
        "        self.conv1 = nn.Conv2d(3, in_channel, 7, padding = 3, stride = 2) #shoudl take 224 to 112\n",
        "        self.avgpool1 = nn.AvgPool2d(3, padding = 1, stride = 2)\n",
        "        \n",
        "        #####################\n",
        "        # making denseblocks\n",
        "        #####################\n",
        "        \n",
        "        self.layer1 = self.make_layer(block, in_channel, k, layers[0], dropout) #1st argument is num of dense blocks\n",
        "        in_channel = int(in_channel+layers[0]*k)\n",
        "        self.trans1 = TransitionBlock(in_channel, int(math.floor(in_channel*theta)), dropout)\n",
        "        in_channel = int(math.floor(in_channel*theta))\n",
        "        \n",
        "        self.layer2 = self.make_layer(block, in_channel, k, layers[1], dropout) #1st argument is num of dense blocks\n",
        "        in_channel = int(in_channel+layers[1]*k)\n",
        "        self.trans2 = TransitionBlock(in_channel, int(math.floor(in_channel*theta)), dropout)\n",
        "        in_channel = int(math.floor(in_channel*theta))\n",
        "\n",
        "        self.layer3 = self.make_layer(block,in_channel, k, layers[2], dropout) #1st argument is num of dense blocks\n",
        "        in_channel = int(in_channel+layers[2]*k)\n",
        "        self.trans3 = TransitionBlock(in_channel, int(math.floor(in_channel*theta)), dropout)\n",
        "        in_channel = int(math.floor(in_channel*theta))\n",
        "        \n",
        "        self.layer4 = self.make_layer(block, in_channel, k, layers[3], dropout) #1st argument is num of dense blocks\n",
        "        in_channel = int(in_channel+layers[3]*k)\n",
        "        \n",
        "        # pooling and classification\n",
        "        self.bn = nn.BatchNorm2d(in_channel)\n",
        "        self.leaky_relu = nn.LeakyReLU(inplace=True)\n",
        "        self.adaptive_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.adaptive_max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.linear = nn.Linear(in_channel*2, n_classes) \n",
        "        \n",
        "        \n",
        "    #####################\n",
        "    # function for making layers\n",
        "    #####################\n",
        "    def make_layer(self, block, in_channel, k, n_layers, dropout):\n",
        "        layers = []\n",
        "        for i in range(n_layers):\n",
        "            layers.append(block(in_channel+i*k, k, dropout))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "        \n",
        "    def forward(self, input):\n",
        "        out = self.avgpool1(self.conv1(input))\n",
        "        out = self.layer1(out)\n",
        "        out = self.trans1(out)\n",
        "        out = self.trans2(self.layer2(out))\n",
        "        out = self.trans3(self.layer3(out))\n",
        "        out = self.leaky_relu(self.bn(self.layer4(out)))\n",
        "        out_a = self.adaptive_avg_pool(out)\n",
        "        out_a = out_a.view(out_a.size(0), -1) \n",
        "        out_b = self.adaptive_max_pool(out)\n",
        "        out_b = out_b.view(out_b.size(0), -1) \n",
        "        \n",
        "        out = torch.cat([out_a, out_b],1)\n",
        "        out = self.linear(out) # output layer\n",
        "\n",
        "        \n",
        "        return out\n",
        "    \n",
        "\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T8zNSv1A0RKe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net = DenseNet(DenseBlock, layers = [1,1,1,1], dropout=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aPO-x1iu0RKh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# tests\n",
        "T = TransitionBlock(3,3)\n",
        "D = DenseBlock(3, 3)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oylh8E2R0RKj",
        "colab_type": "code",
        "colab": {},
        "outputId": "8cfb8fbc-45e3-4c6a-973d-77abb9b4257e"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#summary\n",
        "\n",
        "\n",
        "\n",
        "#summary(net, (3, 224, 224))\n",
        "\n",
        "#summary(D, (3, 224, 224))\n",
        "\n",
        "summary(T, (3, 224, 224))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "       BatchNorm2d-1          [-1, 3, 224, 224]               6\n",
            "         LeakyReLU-2          [-1, 3, 224, 224]               0\n",
            "            Conv2d-3          [-1, 3, 224, 224]               9\n",
            "         AvgPool2d-4          [-1, 3, 112, 112]               0\n",
            "================================================================\n",
            "Total params: 15\n",
            "Trainable params: 15\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ewjFnR_p0RKo",
        "colab_type": "code",
        "colab": {},
        "outputId": "a5dbf56b-5516-4be3-fcec-023416b10a9e"
      },
      "cell_type": "code",
      "source": [
        "summary(net, (3, 224, 224))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 24, 112, 112]           3,552\n",
            "         AvgPool2d-2           [-1, 24, 56, 56]               0\n",
            "       BatchNorm2d-3           [-1, 24, 56, 56]              48\n",
            "         LeakyReLU-4           [-1, 24, 56, 56]               0\n",
            "            Conv2d-5           [-1, 48, 56, 56]           1,152\n",
            "       BatchNorm2d-6           [-1, 48, 56, 56]              96\n",
            "         LeakyReLU-7           [-1, 48, 56, 56]               0\n",
            "            Conv2d-8           [-1, 12, 56, 56]           5,184\n",
            "        DenseBlock-9           [-1, 36, 56, 56]               0\n",
            "      BatchNorm2d-10           [-1, 36, 56, 56]              72\n",
            "        LeakyReLU-11           [-1, 36, 56, 56]               0\n",
            "           Conv2d-12           [-1, 18, 56, 56]             648\n",
            "        AvgPool2d-13           [-1, 18, 28, 28]               0\n",
            "  TransitionBlock-14           [-1, 18, 28, 28]               0\n",
            "      BatchNorm2d-15           [-1, 18, 28, 28]              36\n",
            "        LeakyReLU-16           [-1, 18, 28, 28]               0\n",
            "           Conv2d-17           [-1, 48, 28, 28]             864\n",
            "      BatchNorm2d-18           [-1, 48, 28, 28]              96\n",
            "        LeakyReLU-19           [-1, 48, 28, 28]               0\n",
            "           Conv2d-20           [-1, 12, 28, 28]           5,184\n",
            "       DenseBlock-21           [-1, 30, 28, 28]               0\n",
            "      BatchNorm2d-22           [-1, 30, 28, 28]              60\n",
            "        LeakyReLU-23           [-1, 30, 28, 28]               0\n",
            "           Conv2d-24           [-1, 15, 28, 28]             450\n",
            "        AvgPool2d-25           [-1, 15, 14, 14]               0\n",
            "  TransitionBlock-26           [-1, 15, 14, 14]               0\n",
            "      BatchNorm2d-27           [-1, 15, 14, 14]              30\n",
            "        LeakyReLU-28           [-1, 15, 14, 14]               0\n",
            "           Conv2d-29           [-1, 48, 14, 14]             720\n",
            "      BatchNorm2d-30           [-1, 48, 14, 14]              96\n",
            "        LeakyReLU-31           [-1, 48, 14, 14]               0\n",
            "           Conv2d-32           [-1, 12, 14, 14]           5,184\n",
            "       DenseBlock-33           [-1, 27, 14, 14]               0\n",
            "      BatchNorm2d-34           [-1, 27, 14, 14]              54\n",
            "        LeakyReLU-35           [-1, 27, 14, 14]               0\n",
            "           Conv2d-36           [-1, 13, 14, 14]             351\n",
            "        AvgPool2d-37             [-1, 13, 7, 7]               0\n",
            "  TransitionBlock-38             [-1, 13, 7, 7]               0\n",
            "      BatchNorm2d-39             [-1, 13, 7, 7]              26\n",
            "        LeakyReLU-40             [-1, 13, 7, 7]               0\n",
            "           Conv2d-41             [-1, 48, 7, 7]             624\n",
            "      BatchNorm2d-42             [-1, 48, 7, 7]              96\n",
            "        LeakyReLU-43             [-1, 48, 7, 7]               0\n",
            "           Conv2d-44             [-1, 12, 7, 7]           5,184\n",
            "       DenseBlock-45             [-1, 25, 7, 7]               0\n",
            "      BatchNorm2d-46             [-1, 25, 7, 7]              50\n",
            "        LeakyReLU-47             [-1, 25, 7, 7]               0\n",
            "AdaptiveAvgPool2d-48             [-1, 25, 1, 1]               0\n",
            "AdaptiveMaxPool2d-49             [-1, 25, 1, 1]               0\n",
            "           Linear-50                   [-1, 10]             510\n",
            "================================================================\n",
            "Total params: 30,367\n",
            "Trainable params: 30,367\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NUgoOUVH0RKr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# weight initialization\n",
        "# new init weight        \n",
        "def init_weight(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight) #he initialize, can use xavier instead\n",
        "        #nn.init.constant_(m.bias, 0.001) # optional bias\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_normal_(m.weight) #he initialize, can use xavier instead\n",
        "        #nn.init.constant_(m.bias, 0.001) # optional bias\n",
        "    elif type(m) == nn.BatchNorm2d:\n",
        "        torch.nn.init.constant_(m.weight, 1)\n",
        "        torch.nn.init.constant_(m.bias, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J8TpiUWt0RKt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# apply initializers\n",
        "net.apply(init_weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OsjqWehH0RKw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define loss and optimizer\n",
        "import torch.optim as optim\n",
        "learning_rate = 0.01\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr = learning_rate, momentum=0.9, nesterov= True, weight_decay= 0.01)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qpybaz9_0RKz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# LR scheduler\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "\n",
        "class CosineAnnealingLR_with_Restart(_LRScheduler):\n",
        "    \"\"\"Set the learning rate of each parameter group using a cosine annealing\n",
        "    schedule, where :math:`\\eta_{max}` is set to the initial lr and\n",
        "    :math:`T_{cur}` is the number of epochs since the last restart in SGDR:\n",
        "\n",
        "    .. math::\n",
        "\n",
        "        \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 +\n",
        "        \\cos(\\frac{T_{cur}}{T_{max}}\\pi))\n",
        "\n",
        "    When last_epoch=-1, sets initial lr as lr.\n",
        "\n",
        "    It has been proposed in\n",
        "    `SGDR: Stochastic Gradient Descent with Warm Restarts`_. The original pytorch\n",
        "    implementation only implements the cosine annealing part of SGDR,\n",
        "    I added my own implementation of the restarts part.\n",
        "    \n",
        "    Args:\n",
        "        optimizer (Optimizer): Wrapped optimizer.\n",
        "        T_max (int): Maximum number of iterations. (LENGTH OF 1 CYCLE)\n",
        "        T_mult (float): Increase T_max by a factor of T_mult\n",
        "        eta_min (float): Minimum learning rate. Default: 0.\n",
        "        last_epoch (int): The index of last epoch. Default: -1.\n",
        "        model (pytorch model): The model to save.\n",
        "        out_dir (str): Directory to save snapshots\n",
        "        take_snapshot (bool): Whether to save snapshots at every restart\n",
        "\n",
        "    .. _SGDR\\: Stochastic Gradient Descent with Warm Restarts:\n",
        "        https://arxiv.org/abs/1608.03983\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, optimizer, T_max, T_mult, model, out_dir, take_snapshot, eta_min=0, last_epoch=-1):\n",
        "        self.T_max = T_max\n",
        "        self.T_mult = T_mult\n",
        "        self.Te = self.T_max\n",
        "        self.eta_min = eta_min\n",
        "        self.current_epoch = last_epoch\n",
        "        \n",
        "        self.model = model\n",
        "        self.out_dir = out_dir\n",
        "        self.take_snapshot = take_snapshot\n",
        "        \n",
        "        self.lr_history = []\n",
        "        \n",
        "        super(CosineAnnealingLR_with_Restart, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        new_lrs = [self.eta_min + (base_lr - self.eta_min) *\n",
        "                (1 + math.cos(math.pi * self.current_epoch / self.Te)) / 2\n",
        "\n",
        "                for base_lr in self.base_lrs]\n",
        "        \n",
        "        self.lr_history.append(new_lrs)\n",
        "        return new_lrs\n",
        "    \n",
        "    def step(self, epoch=None):\n",
        "        if epoch is None:\n",
        "        \n",
        "            epoch = self.last_epoch + 1\n",
        "        self.last_epoch = epoch\n",
        "        self.current_epoch += 1\n",
        "        \n",
        "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
        "            param_group['lr'] = lr\n",
        "        \n",
        "        ## restart\n",
        "        if self.current_epoch == self.Te:\n",
        "            print(\"restart at epoch {:03d}\".format(self.last_epoch + 1))\n",
        "            \n",
        "            if self.take_snapshot:\n",
        "                torch.save({\n",
        "                    'epoch': self.T_max,\n",
        "                    'state_dict': self.model.state_dict()\n",
        "                }, self.out_dir + \"/\" + 'snapshot_e_{:03d}.pth.tar'.format(self.T_max))\n",
        "            \n",
        "            ## reset epochs since the last reset\n",
        "            self.current_epoch = 0\n",
        "            \n",
        "            ## reset the next goal\n",
        "            self.Te = int(self.Te * self.T_mult)\n",
        "            self.T_max = self.T_max + self.Te"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vTqqSPzw0RK2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# T_max = how many Epochs before restarting learning rate\n",
        "# T_mult = increase cycle length after restart \n",
        "\n",
        "# try:\n",
        "# 1st training cycle: T_max = 3, T_mult = 1 for 3 cycles (9 epochs)\n",
        "# 2nd training cycle: T_max = 3, T_mult = 2 for 3 cycles (21 epochs)\n",
        "\n",
        "scheduler = CosineAnnealingLR_with_Restart(optimizer, T_max=2, T_mult=2, model = net,  out_dir='blank', take_snapshot=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2njsc51I0RK7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# modified model training to keep track of train/val loss\n",
        "n_epochs = 6\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    scheduler.step()\n",
        "    running_loss = 0.0\n",
        "    total_train_loss = 0.0\n",
        "    for i, train_data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = train_data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print loss per n minibatches\n",
        "        running_loss += loss.item()\n",
        "        total_train_loss += loss.item()\n",
        "        if i % 500 == 499:    # print every 500 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 500))\n",
        "            running_loss = 0.0\n",
        "    \n",
        "    # keep track of loss in test dataset \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for test_data in testloader:\n",
        "            test_images, test_labels = test_data\n",
        "            test_outputs = net(test_images)\n",
        "            test_loss = criterion(test_outputs, test_labels)\n",
        "            total_test_loss += test_loss.item()\n",
        "            _, predicted = torch.max(test_outputs.data, 1)\n",
        "            total += test_labels.size(0)\n",
        "            correct += (predicted == test_labels).sum().item()\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    # for printing average loss every epoch\n",
        "    print(\"===> Epoch {} Complete: Train Avg. Loss: {:.4f}\".format(epoch+1, total_train_loss / len(trainloader)))\n",
        "    print(\"===> Epoch {} Complete: Test Avg. Loss: {:.4f}\".format(epoch+1, total_test_loss / len(testloader)))\n",
        "    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5MExUtlg0RK_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}